{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOez1hXHZAtcv5wUd/Yj+Xg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vhrique/anne_ptbr/blob/main/A04_Linguagem_Natural.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Histórico de Redes Neurais para Processamento de Linguagem Natural\n",
        "\n",
        "O processamento de linguagem natural e as redes neurais se desenvolveram de maneira profundamente interligada nos últimos anos. A demanda por modelos capazes de lidar com a natureza sequencial e dependente de dados de texto impulsionando avanços significativos na área de redes neurais. No início, abordagens tradicionais, como algoritmos baseados em regras e modelos estatísticos baseados em conjuntos de palavras, eram amplamente utilizados para o processamento de linguagem natural. No entanto, essas técnicas apresentavam dificuldades em capturar dependências de longo prazo e padrões complexos. A introdução de modelos de linguagem neurais, aliado com redes neurais recorrentes (RNNs) como as LSTMs (Long Short-Term Memory) e GRUs (Gated Recurrent Units), permitiram o aprendizado de dependências mais longas, tornando-se padrões em aplicações como tradução automática. Mais recentemente, o surgimento de arquiteturas baseadas em atenção, como os Transformers, revolucionou ainda mais o campo, eliminando a necessidade de processamento sequencial estrito e alcançando resultados de ponta. Essa coevolução recente entre redes neurais e o processamento de dados sequenciais permitiu avanços que moldaram profundamente a inteligência artificial moderna."
      ],
      "metadata": {
        "id": "0NR9C5yxjvkh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Primeiros Modelos Neurais"
      ],
      "metadata": {
        "id": "yAJYrU5Nxb9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Redes Neurais Recorrentes\n",
        "\n",
        "As Redes Neurais Recorrentes (RNNs) são uma classe de redes projetadas especificamente para processar dados sequenciais, como séries temporais, texto ou áudio. Ao contrário das redes tradicionais, as RNNs têm conexões que permitem que a saída de uma unidade seja usada como entrada para a próxima, criando uma memória sequencial que ajuda a capturar dependências entre os elementos da sequência. Isso as torna particularmente atraentes para tarefas como tradução automática e reconhecimento de fala."
      ],
      "metadata": {
        "id": "QA3IsL6YQNUY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No entanto, RNNs simples podem sofrer com problemas de gradiente desvanecente, dificultando o aprendizado de dependências de longo prazo. Para lidar com isso, variantes como LSTM (Long Short-Term Memory) e GRU (Gated Recurrent Unit) foram desenvolvidas, introduzindo mecanismos que controlam o fluxo de informações e preservam a memória por períodos mais longos, tornando-as mais eficazes em capturar padrões complexos em dados sequenciais."
      ],
      "metadata": {
        "id": "PdvBrbOOQQe0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM (1997)\n",
        "\n",
        "A LSTM (Long Short-Term Memory) é uma variante avançada das RNNs, projetada para lidar com o problema do gradiente desvanecente, que afeta as redes recorrentes clássicas quando tentam aprender dependências de longo prazo. Introduzida por Hochreiter e Schmidhuber em 1997, a LSTM utiliza um sistema de portas (entrada, esquecimento e saída) que controla o fluxo de informações em cada célula da rede. A LSTM foi um avanço crucial no campo de redes recorrentes, permitindo um aprendizado mais eficiente e robusto em sequências longas, trazendo avanços em modelagem de linguagem natural com redes neurais.\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/lstm.png?raw=true\" width=500></center>"
      ],
      "metadata": {
        "id": "yu8g4hKfQV2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRU (2014)\n",
        "\n",
        "A GRU (Gated Recurrent Unit) é uma variante das redes LSTM, proposta por Cho et al. em 2014, que simplifica o design das LSTMs, mantendo muitos de seus benefícios. Diferente da LSTM, a GRU combina as funcionalidades das portas de entrada e de esquecimento em uma única porta de atualização, e usa uma porta de reset para controlar a quantidade de informação que flui do estado anterior para o atual. Essa simplificação torna a GRU mais eficiente em termos de computação e memória, já que ela tem menos parâmetros do que a LSTM, sem sacrificar o desempenho em muitas tarefas. A GRU é particularmente eficaz para capturar dependências temporais de curto e longo prazo, tornando-a uma escolha popular em aplicações como tradução automática, modelagem de séries temporais e reconhecimento de fala. Devido à sua simplicidade e desempenho comparável ao da LSTM, a GRU é frequentemente usada quando há necessidade de redes mais leves e rápidas.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/gru.png?raw=true\" width=400></center>"
      ],
      "metadata": {
        "id": "_LkQgCM1Qa0Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Redes Bidirecionais\n",
        "\n",
        "As BiLSTM (Bidirectional Long Short-Term Memory) e BiGRU (Bidirectional Gated Recurrent Unit) são versões bidirecionais das arquiteturas LSTM e GRU, projetadas para capturar informações tanto do passado quanto do futuro em uma sequência de dados. Em vez de processar os dados apenas em uma direção (do início ao fim), essas redes utilizam duas camadas recorrentes: uma que processa a sequência na ordem tradicional e outra que processa na ordem inversa (Schuster et al., 1997). Essa abordagem permite que as redes bidirecionais captem dependências contextuais de ambas as direções, o que é especialmente útil em tarefas como tradução automática, onde o significado de uma palavra pode depender tanto do contexto anterior quanto do posterior, ou em reconhecimento de fala, onde a compreensão de um som pode ser influenciada por fonemas adjacentes. A BiLSTM é mais flexível, devido ao seu controle mais refinado de memória com as três portas, enquanto a BiGRU, por ser mais simples e eficiente, é preferida em cenários onde o desempenho computacional é uma prioridade. Ambas são amplamente usadas em processamento de linguagem natural e tarefas sequenciais complexas.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/birnn.png?raw=true\" width=500></center>"
      ],
      "metadata": {
        "id": "ggSWJ1znjeTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seq2Seq\n",
        "\n",
        "As arquiteturas Seq2Seq (Sequence-to-Sequence) são redes neurais projetadas para transformar uma sequência de entrada em outra sequência de saída, sendo amplamente utilizadas em tarefas como tradução automática, resumo de texto e reconhecimento de fala. A arquitetura típica envolve um encoder e um decoder, ambos normalmente compostos por redes neurais recorrentes (RNNs), como LSTMs ou GRUs, ou versões mais recentes com Transformers. O encoder processa a sequência de entrada e gera uma representação vetorial (um estado oculto), que captura as informações relevantes da entrada. O decoder, por sua vez, utiliza essa representação para gerar a sequência de saída, um elemento por vez. A transição entre o encoder e o decoder pode ser facilitada por um mecanismo de atenção, que permite que o decoder se concentre em diferentes partes da entrada enquanto gera cada elemento da saída, melhorando a precisão em tarefas complexas, como tradução.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/seq2seq.png?raw=true\" width=600></center>"
      ],
      "metadata": {
        "id": "oqmi-QKg_6eF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Desafios de Redes Neurais Recorrentes\n",
        "\n",
        "As RNNs apresentam desafios importantes, principalmente quando se trata de capturar dependências de longo prazo em dados sequenciais. Devido ao problema do gradiente desvanecente, as RNNs tendem a perder informações importantes conforme as dependências se tornam mais distantes ao longo da sequência, resultando em um aprendizado menos eficaz. Além disso, o processamento sequencial, onde cada etapa depende da anterior, torna o treinamento computacionalmente caro e difícil de paralelizar, limitando a eficiência da rede em grandes volumes de dados. Essas limitações prejudicaram o desempenho das RNNs em tarefas que requerem a análise de contextos mais longos e levaram à busca por soluções mais avançadas.\n",
        "\n"
      ],
      "metadata": {
        "id": "0NabXAVUkvvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mecanismos de Atenção (2014)\n",
        "\n",
        "Para superar as limitações das RNNs tradicionais, foi introduzido o mecanismo de atenção, que permite que a rede \"preste atenção\" em diferentes partes da sequência, independentemente da posição (Bahdanau et al., 2014). O mecanismo de atenção atua como um complemento ao processo recorrente, permitindo que a rede atribua pesos diferentes a diferentes elementos da entrada e capture informações relevantes, mesmo que estejam distantes na sequência. Isso melhora significativamente a capacidade das RNNs de lidar com dependências de longo alcance. Aplicações como a atenção em tradução automática demonstraram que esse mecanismo é eficaz para focar em palavras específicas de uma sentença ao traduzir, superando parte das limitações das RNNs puras.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/bahdanau.JPG?raw=true\"></center>\n",
        "\n",
        "Na arquitetura de Bahdanu (2014) mostrada acima, vemos um modelo Sequência a Sequência (_Sequence to Sequence_ - Seq2seq), onde temos um codificador e um decodificador. No exemplo, o codificador é uma RNN bidirecional que extrai características $h_i$ para cada token $x_i$ na sequência de entrada $\\textbf{x}$ com comprimento $T$. O decodificador é uma RNN que calcula a saída $y_i$ de forma iterativa, passo-a-passo, utilizando o estado interno $s_i$, a saída anterior $y_{i-1}$ e um contexto $c_i$, enquanto o estado interno $s_i$ é uma função da saída anterior $y_{i-1}$, o estado anterior $s_{i-1}$ e o contexto $c_i$. Este contexto $c_i$ é uma média ponderada de todas as características $h_i$ calculadas pelo codificador, em função de pesos de atenção $a_i$. Cada um dos pesos de atenção $a_i$ é calculado como uma função das características $h_i$ e o estado interno do decodificador $s_{i-1}$. Neste caso, todas as funções são redes neurais aprendidas com os próprios dados."
      ],
      "metadata": {
        "id": "Y3bdrIbEQjcR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evolução em Processamento de Dados Sequenciais\n",
        "\n",
        "Com base no sucesso dos mecanismos de atenção, os Transformers, introduzidos em 2017 por Vasvani et al., levaram essa ideia ao próximo nível ao eliminar completamente a necessidade de recorrência. Os Transformers utilizam atenção auto-regressiva para processar toda a sequência de dados de uma vez, permitindo que a rede identifique dependências de curto e longo prazo de maneira eficiente, sem os problemas de gradientes desvanecentes. Isso também permite uma alta paralelização do processamento, acelerando consideravelmente o treinamento em grandes volumes de dados. Desde sua introdução, os Transformers se tornaram a arquitetura dominante em tarefas de processamento de linguagem natural e outras áreas, como visão computacional, devido à sua flexibilidade e eficiência."
      ],
      "metadata": {
        "id": "h4gH4FSHQhtZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers (2017)\n",
        "\n",
        "A arquitetura do Transformer é baseada inteiramente em mecanismos de atenção, sem o uso de recorrência ou convolução. Ela consiste em dois blocos principais: um encoder e um decoder, ambos compostos de múltiplas camadas. Cada camada do encoder é formada por duas subcamadas principais: atenção auto-regressiva (self-attention), que permite que a rede atribua pesos a diferentes partes da sequência de entrada, e uma camada feed-forward que processa as características extraídas. O decoder segue uma estrutura similar, com a adição de uma subcamada de atenção cruzada que se conecta ao encoder, além de sua própria atenção auto-regressiva. Uma característica chave do Transformer é o uso de embeddings posicionais, que adicionam informações sobre a ordem dos elementos da sequência, algo que as RNNs capturam de forma nativa, mas que o Transformer precisa compensar. Essa estrutura permite que o Transformer processe sequências inteiras de forma paralelizada, tornando-o extremamente eficiente em tarefas como tradução automática, processamento de linguagem natural e até visão computacional.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/transformer.png?raw=true\" width=300></center>\n",
        "\n",
        "### Self-Attention\n",
        "\n",
        "A self-attention (atenção auto-regressiva) é um mecanismo usado para permitir que modelos atribuam pesos diferentes a diferentes partes de uma sequência de entrada, dependendo da relevância contextual entre elas. Em vez de processar uma sequência de dados de forma sequencial, a self-attention permite que o modelo \"preste atenção\" em todas as posições da sequência simultaneamente, calculando a importância de cada elemento em relação aos outros. Isso é feito ao gerar três vetores para cada token da sequência: query (consulta), key (chave) e value (valor). O modelo calcula as similaridades entre as queries e keys, produzindo uma pontuação de atenção, que é usada para ponderar os valores correspondentes. Com o self-attention, o modelo pode identificar relações importantes em toda a sequência, de maneira eficiente e paralelizada.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/self-attention.png?raw=true\" width=600></center>\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/attention_fix.jpg?raw=true\" width=700></center>\n",
        "\n",
        "### Layer Normalization\n",
        "\n",
        "Os Transformers lidam com sequências de comprimento variável, como frases ou parágrafos, onde cada token depende de sua posição na sequência. O BatchNorm, que normaliza as ativações com base em todo o lote, pode interferir nessas dependências temporais. Por outro lado, o LayerNorm normaliza cada amostra de forma independente, sem depender de outras amostras no lote, preservando assim as dependências temporais e contextuais dentro das sequências.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/layernorm.jpg?raw=true\" width=300></center>\n",
        "\n",
        "A Layer Normalization (LayerNorm) foi introduzida em 2016, em um artigo intitulado \"Layer Normalization\" por Jimmy Lei Ba, Jamie Ryan Kiros e Geoffrey Hinton. A técnica foi desenvolvida para normalizar as ativações das camadas de redes neurais de forma independente em cada amostra, ao invés de calcular as estatísticas em lotes de dados como no Batch Normalization.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i8GK6vH0p3JR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avanços em Processamento de Linguagem Natural\n",
        "\n",
        "O Transformer revolucionou o processamento de linguagem natural (NLP) ao substituir arquiteturas sequenciais, como RNNs e LSTMs, por um mecanismo de atenção auto-regressiva que processa todo o contexto de uma sequência simultaneamente e de maneira paralela. Isso não apenas eliminou as limitações de capturar dependências de longo prazo, mas também permitiu uma paralelização eficiente do treinamento, acelerando significativamente o processamento em grandes volumes de dados. Modelos baseados no Transformer, como BERT e GPT, redefiniram o estado da arte em tarefas de NLP, como tradução, resposta a perguntas e geração de texto, graças à sua capacidade de capturar contextos complexos de maneira bidirecional ou autoregressiva.\n",
        "\n",
        "\n",
        "### BERT (2018)\n",
        "\n",
        "O BERT (Bidirectional Encoder Representations from Transformers) é uma aplicação avançada da arquitetura Transformer, especificamente do encoder do Transformer, desenvolvido pela Google em 2018. Enquanto o Transformer completo possui um componente de encoder e decoder para tarefas como tradução, o BERT utiliza apenas o encoder, focando em capturar o contexto bidirecional de uma palavra em uma sequência de texto. Isso significa que BERT analisa cada palavra levando em consideração as palavras à esquerda e à direita, ao contrário de modelos unidirecionais que só olham em uma direção. O uso dessa arquitetura Transformer bidirecional permite ao BERT obter representações de texto altamente contextualizadas, tornando-o extremamente eficaz para tarefas de processamento de linguagem natural, como perguntas e respostas, reconhecimento de entidades e análise de sentimento.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/bert.jpg?raw=true\" width=700></center>\n",
        "\n",
        "BERT é treinado usando duas tarefas principais: máscara de palavras (Masked Language Modeling), onde o modelo tenta prever palavras escondidas em uma sentença, e previsão de sentenças adjacentes (Next Sentence Prediction), onde ele prevê se uma frase segue logicamente a outra. Essa abordagem permite ao BERT capturar nuances complexas de significado e relações contextuais profundas, o que o torna altamente eficaz em tarefas como classificação de texto, resposta a perguntas e tradução automática.\n",
        "\n",
        "### GPT (2018-2023)\n",
        "\n",
        "O GPT-1 (Generative Pre-trained Transformer) foi o primeiro modelo da série GPT, introduzido pela OpenAI em 2018. Ele se baseia na arquitetura do decoder do Transformer, utilizando uma abordagem unidirecional para processar texto, ou seja, cada token é gerado com base apenas nos tokens anteriores da sequência. Ao contrário do BERT, que é bidirecional, o GPT-1 foca na tarefa de modelagem de linguagem em que o modelo é pré-treinado para prever a próxima palavra em uma sequência de texto. O GPT-1 foi pré-treinado em uma grande quantidade de dados não supervisionados, sendo posteriormente ajustado para tarefas específicas de NLP, como tradução e classificação, com um processo simples de fine-tuning. Sua principal inovação foi a demonstração de que o pré-treinamento de modelos de linguagem em grandes volumes de dados seguido por ajuste fino pode produzir excelentes resultados em diversas tarefas de linguagem natural.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/gpt.JPG?raw=true\" width=700></center>\n",
        "\n",
        "Com o lançamento de versões mais avançadas, como GPT-3 e GPT-4, o modelo se destacou pela sua capacidade de realizar uma ampla gama de tarefas, como tradução, resposta a perguntas, escrita criativa e geração de código, sem precisar de grandes quantidades de dados anotados. O uso de zero-shot e few-shot learning com GPT permitiu ao modelo generalizar para novas tarefas apenas com descrições mínimas ou alguns exemplos, transformando a maneira como abordamos o processamento de linguagem natural e outras aplicações de IA."
      ],
      "metadata": {
        "id": "k5Alyw8qQsy7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avanços em Visão Computacional\n",
        "\n",
        "Os Transformers também trouxeram avanços significativos para a visão computacional, especialmente com a introdução do Vision Transformer (ViT). O ViT adaptou o mecanismo de atenção auto-regressiva dos Transformers, originalmente desenvolvido para processamento de linguagem natural, para processar imagens de forma mais eficiente. Esses avanços permitiram aos Transformers superarem redes convolucionais tradicionais em diversas tarefas de visão computacional, como reconhecimento de objetos, segmentação semântica e detecção de anomalias, especialmente quando grandes volumes de dados estão disponíveis para o treinamento.\n",
        "\n",
        "### Vision Transformer (2020)\n",
        "\n",
        "O Vision Transformer (ViT) foi uma inovação no campo da visão computacional, ao adaptar o modelo de Transformers para processar imagens em vez de texto. Introduzido por Dosovitskiy et al. em 2020, o ViT substitui as convoluções tradicionais ao dividir uma imagem em pequenos patches e tratá-los como sequências de tokens, semelhantes às palavras em um texto. Utilizando o mecanismo de self-attention, o ViT é capaz de capturar relações globais e locais entre diferentes partes da imagem, permitindo uma compreensão mais rica do conteúdo visual. Essa abordagem mostrou-se altamente eficaz, especialmente em grandes conjuntos de dados, onde o ViT superou as redes convolucionais tradicionais em tarefas como classificação de imagens e segmentação. A simplicidade do design e a capacidade de paralelizar o treinamento são vantagens significativas, tornando o ViT uma arquitetura revolucionária no campo da visão computacional.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/ViT.png?raw=true\" width=500></center>\n",
        "\n",
        "### DETR (2020)\n",
        "\n",
        "DETR (Detection Transformer) é uma arquitetura inovadora para detecção de objetos que combina redes neurais com o mecanismo de Transformers (Carion et al, 2020). Ao contrário das abordagens tradicionais baseadas em convoluções e region proposals, o DETR trata a detecção de objetos como um problema de correspondência entre um conjunto fixo de previsões e objetos presentes na imagem. Ele usa um backbone convolucional para extrair características da imagem, seguido por camadas de Transformers que modelam as relações globais entre as características e produzem previsões diretas para as caixas delimitadoras e classificações. DETR é único por dispensar o uso de heurísticas como Non-Maximum Suppression (NMS) e propostas de regiões, proporcionando uma abordagem simplificada e direta. Apesar de ser mais lento no treinamento inicial, o DETR demonstra forte desempenho em termos de precisão e generalização, especialmente em cenários complexos com sobreposição de objetos.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/detr_2.png?raw=true\" width=800></center>\n",
        "\n",
        "O DETR utiliza três componentes principais em sua função de perda: a Matching Loss, baseada no algoritmo de Hungarian matching, que faz a correspondência entre as previsões e os objetos reais; a perda de classificação, que usa entropia cruzada para penalizar previsões incorretas de classe; e a perda de regressão das caixas delimitadoras, que combina L1 loss para ajustar as coordenadas das caixas e generalized IoU loss para melhorar o alinhamento entre as caixas preditas e as reais. Essas perdas são otimizadas em conjunto para garantir precisão tanto na classificação quanto na localização dos objetos.\n",
        "\n",
        "### Swin Transformer (2021)\n",
        "\n",
        "O Swin Transformer é uma evolução dos Transformers aplicados à visão computacional, projetado para melhorar a eficiência do Vision Transformer (ViT), especialmente em imagens de alta resolução. Introduzido por Liu et al. em 2021, o Swin Transformer utiliza uma abordagem hierárquica com janelas deslizantes (sliding windows), onde o mecanismo de self-attention é aplicado localmente dentro dessas janelas. Isso permite uma redução significativa no custo computacional, mantendo a capacidade de capturar relações entre diferentes partes da imagem. À medida que a rede avança, as janelas se expandem hierarquicamente, permitindo que o modelo capture tanto características locais quanto contextos globais de maneira eficiente. O Swin Transformer tem se destacado em diversas tarefas de visão, como segmentação de imagens, detecção de objetos e reconhecimento de cenas, oferecendo uma combinação poderosa de precisão e eficiência computacional.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/swin-transformer.png?raw=true\" width=500></center>\n",
        "\n",
        "### Segment Anything Model (2023)\n",
        "\n",
        "O Segment Anything Model (SAM), desenvolvido pela Meta AI (Kirillov et al., 2023), é uma rede neural projetada para realizar segmentação generalista em qualquer imagem com zero ou poucos ajustes, tornando-se uma ferramenta poderosa para uma ampla gama de aplicações. Diferente dos métodos tradicionais que exigem treinamento específico para cada tarefa ou conjunto de dados, o SAM foi treinado em um grande volume de dados com imagens e máscaras, permitindo segmentar qualquer objeto em qualquer contexto com base em prompts simples, como cliques, caixas delimitadoras ou texto. Sua arquitetura combina técnicas avançadas, como Vision Transformers (ViTs), para produzir máscaras de alta qualidade em diferentes resoluções, sem a necessidade de conhecimento específico do objeto a ser segmentado. O SAM representa um avanço na segmentação universal, democratizando o acesso a técnicas de segmentação automática para uma variedade de domínios.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/sam.JPG?raw=true\" width=700></center>"
      ],
      "metadata": {
        "id": "WwubgE61Qv-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avanços em Escalabilidade\n",
        "\n",
        "Os Transformers, apesar de sua eficiência em capturar dependências globais, enfrentam desafios de escalabilidade, especialmente com dados de alta dimensão, como imagens e vídeos, devido ao alto custo computacional da auto-atenção, que cresce quadraticamente com o tamanho da entrada. Uma inovação é o Flash Attention, que otimiza o cálculo da auto-atenção ao reduzir o uso de memória e acelerar o processamento em GPUs. Esse tipo de avanço está tornando os Transformers mais eficientes e escaláveis para uma gama maior de aplicações e volumes de dados. Alguns exemplos seguem abaixo.\n",
        "\n",
        "### Linformer\n",
        "\n",
        "O Linformer é uma variante dos Transformers que foi proposta para reduzir a complexidade computacional do cálculo de auto-atenção, que tradicionalmente cresce de forma quadrática com o número de tokens de entrada. A inovação do Linformer é a ideia de aproximar as matrizes de atenção de uma forma que elas tenham uma dimensão fixa, independentemente do comprimento da sequência de entrada. Isso é feito por meio de uma projeção linear das sequências, o que reduz o custo computacional da auto-atenção para complexidade linear. Essa abordagem torna o Linformer significativamente mais eficiente, especialmente em tarefas que envolvem sequências longas, como processamento de linguagem natural e visão computacional, sem comprometer a precisão dos modelos. O Linformer é particularmente útil para ambientes com restrições de recursos e para treinar modelos em grandes conjuntos de dados com alta eficiência.\n",
        "\n",
        "### Reformer\n",
        "\n",
        "O Reformer é uma arquitetura otimizada de Transformers, proposta para lidar com os desafios de escalabilidade e eficiência no processamento de grandes sequências. O Reformer introduz duas inovações principais para reduzir a complexidade computacional e o uso de memória. A primeira é o uso de Locality-Sensitive Hashing (LSH) na camada de auto-atenção, que aproxima a atenção ao comparar tokens similares de forma eficiente, reduzindo a complexidade da auto-atenção de quadrática para subquadrática. A segunda inovação é o uso de codificação reversível, o que elimina a necessidade de armazenar os estados intermediários de cada camada durante o treinamento, reduzindo significativamente o uso de memória. Essas melhorias tornam o Reformer capaz de processar sequências muito longas, como textos extensos ou vídeos, de forma mais escalável e eficiente, enquanto mantém a precisão dos Transformers tradicionais.\n",
        "\n",
        "### Big Bird\n",
        "\n",
        "O Big Bird é uma variante dos Transformers projetada para lidar com sequências extremamente longas, como documentos completos ou genomas, onde o cálculo tradicional de auto-atenção seria inviável devido ao alto custo computacional. Em vez de usar a atenção plena, o Big Bird introduz uma atenção esparsa que combina três tipos de atenção: local, global e aleatória. A atenção local captura relações próximas entre tokens, a atenção global foca em tokens importantes em posições fixas, e a atenção aleatória cria conexões entre tokens distantes de forma esparsa. Essa combinação reduz a complexidade da auto-atenção de quadrática para linear, permitindo o processamento eficiente de sequências muito maiores sem sacrificar o desempenho. O Big Bird se mostrou eficaz em várias tarefas de processamento de linguagem natural, como classificação de documentos, modelagem de linguagem, e até tarefas em bioinformática, demonstrando sua versatilidade e eficiência para grandes volumes de dados.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/big-bird.png?raw=true\" width=500></center>\n",
        "\n",
        "### Flash Attention\n",
        "\n",
        "Flash Attention é uma técnica recente desenvolvida para otimizar o cálculo do mecanismo de auto-atenção em Transformers, reduzindo o uso de memória e acelerando o processamento. Tradicionalmente, o cálculo da auto-atenção tem um custo computacional quadrático em relação ao número de tokens de entrada, o que torna o processamento de sequências longas caro e ineficiente. O Flash Attention resolve esse problema ao realizar os cálculos de atenção de forma mais eficiente, armazenando somas parciais diretamente na memória e realizando as operações de forma fragmentada. Isso não só reduz o uso de memória como também melhora o desempenho em hardware como GPUs, permitindo que os Transformers processem sequências maiores com menos recursos computacionais. Essa técnica é especialmente útil em modelos grandes e para aplicações em larga escala, como processamento de linguagem natural e visão computacional."
      ],
      "metadata": {
        "id": "LlRXpy1pRhO7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Considerações Finais\n",
        "\n",
        "Na aula, abordamos arquiteturas fundamentais de redes neurais para processamento de sequencias: RNNs, projetadas para lidar com dados sequenciais, como séries temporais e texto, capturando dependências temporais; e Transformers, que revolucionaram o processamento de linguagem natural e, mais recentemente, a visão computacional, ao usar mecanismos de atenção auto-regressiva para capturar relações globais de forma eficiente e paralelizada. Cada uma dessas arquiteturas tem aplicações específicas e vantagens únicas, sendo marcos importantes no desenvolvimento da inteligência artificial.\n",
        "\n",
        "## RNN vs Transformer\n",
        "\n",
        "As RNNs e os Transformers são ambos usados para processar dados sequenciais, mas diferem fundamentalmente em seu funcionamento. As RNNs processam a sequência de dados de forma sequencial, etapa por etapa, o que limita a eficiência e dificulta a captura de dependências de longo prazo devido ao problema do gradiente desvanecente. Já os Transformers utilizam o mecanismo de self-attention, permitindo que o modelo capture dependências em qualquer posição da sequência de forma paralelizada e mais eficiente, superando as limitações das RNNs. Como resultado, os Transformers têm se tornado o padrão em muitas tarefas de processamento de linguagem natural e visão computacional, substituindo as RNNs em diversas aplicações.\n",
        "\n",
        "## CNN vs ViT\n",
        "\n",
        "As CNNs (Redes Neurais Convolucionais) e os Vision Transformers (ViT) são ambas arquiteturas utilizadas para visão computacional, mas diferem na forma como processam imagens. As CNNs utilizam convoluções locais, aplicando filtros para detectar padrões como bordas e texturas em pequenas regiões da imagem, com forte viés indutivo sobre a estrutura espacial. Já o ViT trata a imagem como uma sequência de patches (pequenos blocos), utilizando o mecanismo de self-attention para capturar relações globais e locais de forma mais flexível. As CNNs são mais eficientes em conjuntos de dados menores, mas o ViT se destaca em grandes volumes de dados, onde pode superar as CNNs em termos de desempenho, graças à sua capacidade de modelar dependências globais de maneira eficiente.\n",
        "\n",
        "## MLP vs Transformer\n",
        "\n",
        "As MLPs (Multilayer Perceptrons) e os Transformers são ambas redes neurais, mas têm diferenças fundamentais em como processam dados. O MLP é uma arquitetura mais simples, composta por camadas totalmente conectadas que tratam cada entrada de forma independente, sem considerar a estrutura dos dados. Isso limita sua capacidade de capturar relações complexas ou estruturais entre os elementos dos dados, como sequências temporais ou imagens. Em contraste, os Transformers utilizam o mecanismo de self-attention, permitindo que o modelo capture dependências globais entre diferentes partes de uma sequência, como palavras em um texto ou patches em uma imagem, de forma mais eficiente. Enquanto os MLPs são eficientes em tarefas simples e estruturadas, os Transformers se destacam em tarefas que exigem capturar relações contextuais complexas, como processamento de linguagem natural e visão computacional.\n",
        "\n",
        "## Próxima Aula\n",
        "\n",
        "Na próxima aula, trataremos sobre paradigmas avançados de redes neurais artificiais, como aprendizagem auto-supervisionada, contrastive learning e meta-learning."
      ],
      "metadata": {
        "id": "h7m_6tpES5xC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercícios\n",
        "\n",
        "1. Verifique os exemplos da aula 4 para série temporal e processamento de linguagem natural\n",
        "2. Responda as seguintes perguntas:\n",
        "  1. Porque a LSTM foi mais eficiente que a MLP?\n",
        "  2. Qual a grande desvantagem de Transformers?\n",
        "3. Altere a LSTM por uma GRU no exemplo 4.a. Qual foi o resultado?"
      ],
      "metadata": {
        "id": "vCJSzsBOTNIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referências\n",
        "\n",
        "\n",
        "- Werbos, P. J. (1990). Backpropagation through time: what it does and how to do it. Proceedings of the IEEE, 78(10), 1550-1560.\n",
        "- Hochreiter, S. (1997). Long Short-term Memory. Neural Computation MIT-Press.\n",
        "- Cho, K. (2014). On the Properties of Neural Machine Translation: Encoder-decoder Approaches. arXiv preprint arXiv:1409.1259.\n",
        "- Schuster, M., & Paliwal, K. K. (1997). Bidirectional recurrent neural networks. IEEE transactions on Signal Processing, 45(11), 2673-2681.\n",
        "- Bahdanau, D. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.\n",
        "- Vaswani, A. (2017). Attention is all you need. Advances in Neural Information Processing Systems.\n",
        "- Ba, J. L. (2016). Layer normalization. arXiv preprint arXiv:1607.06450.\n",
        "- Devlin, J. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n",
        "- Radford, A. (2018). Improving language understanding by generative pre-training.\n",
        "- Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsupervised multitask learners. OpenAI blog, 1(8), 9.\n",
        "- Brown, T. B. (2020). Language models are few-shot learners. arXiv preprint arXiv:2005.14165.\n",
        "- Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., ... & McGrew, B. (2023). Gpt-4 technical report. arXiv preprint arXiv:2303.08774.\n",
        "- Dosovitskiy, A. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.\n",
        "- Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., & Zagoruyko, S. (2020, August). End-to-end object detection with transformers. In European conference on computer vision (pp. 213-229). Cham: Springer International Publishing.\n",
        "- Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., ... & Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 10012-10022).\n",
        "- Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., ... & Girshick, R. (2023). Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 4015-4026).\n",
        "- Wang, S., Li, B. Z., Khabsa, M., Fang, H., & Ma, H. (2020). Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768.\n",
        "- Kitaev, N., Kaiser, Ł., & Levskaya, A. (2020). Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451.\n",
        "- Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., ... & Ahmed, A. (2020). Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33, 17283-17297.\n",
        "- Dao, T., Fu, D., Ermon, S., Rudra, A., & Ré, C. (2022). Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35, 16344-16359.\n",
        "\n"
      ],
      "metadata": {
        "id": "fpwSyDT9TPKA"
      }
    }
  ]
}