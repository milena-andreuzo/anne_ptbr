{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMy9AaDjT0oYub3HzJ2a1Yp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vhrique/anne_ptbr/blob/main/04_Arquiteturas_Especializadas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introdução à Arquiteturas Especializadas\n",
        "\n",
        "As arquiteturas clássicas de redes neurais, como as redes densas (MLPs), convolucionais (CNNs) e recorrentes (RNNs), são amplamente usadas para tarefas de classificação e regressão, aproveitando sua estrutura para extrair padrões dos dados e realizar previsões. Essas redes, em geral, aprendem a mapear entradas para saídas rotuladas e são versáteis em diferentes domínios.\n",
        "\n",
        "No entanto, há arquiteturas especializadas que atendem a demandas mais específicas. Em tarefas de visão computacional, arquiteturas voltadas para detecção de objetos e segmentação aprendem a identificar e localizar múltiplas classes em uma imagem, demonstrando a evolução do campo para resolver problemas além da classificação tradicional. Em processamento de linguagem natural, temos diferentes arquiteturas baseadas em sequencias, ou processamento paralelo para codificar e decodificar informação de textos.\n",
        "\n",
        "Nesta aula, falaremos mais sobre estes tipos de arquiteturas e como isto muda em relação à representação, avaliação e otimização de modelos."
      ],
      "metadata": {
        "id": "-9Op2dJky8NF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Arquiteturas Especializadas em Visão Computacional\n",
        "\n",
        "Em visão computacional, tarefas distintas como detecção de objetos, segmentação e geração de imagens exigem combinações específicas de camadas e funções de perda nas arquiteturas de rede. Por exemplo, a detecção de objetos pode combinar camadas convolucionais para extração de características com camadas de regressão para prever posições, utilizando funções de perda que equilibram precisão de classificação e localização. Na segmentação, são empregadas camadas que mantêm a resolução espacial para identificar pixels específicos, com funções de perda que medem a correspondência entre as previsões e as máscaras reais. Dessa forma, a escolha adequada de camadas e funções de perda é crucial para atender aos requisitos de cada tarefa específica."
      ],
      "metadata": {
        "id": "e0v68ocrabjX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detecção de Objetos\n",
        "\n",
        "As arquiteturas para detecção de objetos em visão computacional combinam diversos componentes para identificar e localizar objetos em imagens. Tipicamente, utilizam camadas convolucionais para extrair características relevantes, seguidas por mecanismos que propõem regiões de interesse onde os objetos possivelmente estão. Essas regiões são então refinadas e classificadas através de camadas específicas que prevêem tanto a categoria do objeto quanto a posição das caixas delimitadoras. Funções de perda apropriadas são aplicadas para otimizar simultaneamente a precisão da classificação e a exatidão da localização. A integração eficaz desses componentes é crucial para o desempenho de modelos de detecção de objetos.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/obj_detection.jpg?raw=true\"></center>"
      ],
      "metadata": {
        "id": "hgTI6j4mao_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### R-CNN (2014)\n",
        "\n",
        "R-CNN (Regions with Convolutional Neural Networks) é uma arquitetura pioneira para detecção de objetos em visão computacional (Girshick et al, 2014). Ela funciona gerando propostas de regiões que possivelmente contêm objetos, usando algoritmos como busca seletiva. Cada região proposta é redimensionada e passada por uma rede neural convolucional para extração de características. Essas características são então alimentadas em um classificador, como uma SVM, para determinar a classe do objeto na região. Embora o R-CNN tenha melhorado significativamente a precisão na detecção de objetos, seu processamento é lento, pois cada região proposta precisa ser processada individualmente pela rede neural.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/rcnn.png?raw=true\" width=700></center>\n",
        "\n",
        "A função de perda (loss function) do R-CNN combina duas partes principais: uma perda de classificação e uma perda de regressão para as caixas delimitadoras. A perda de classificação é geralmente uma entropia cruzada que avalia a precisão da previsão da classe do objeto em cada região proposta. Já a perda de regressão avalia a precisão na predição das coordenadas da caixa delimitadora, geralmente usando um erro quadrático ou L1/L2 loss. Essas duas perdas são combinadas para ajustar a rede, garantindo que ela não apenas classifique corretamente os objetos, mas também localize-os de maneira precisa."
      ],
      "metadata": {
        "id": "WxR7386yawlC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fast R-CNN (2015)\n",
        "\n",
        "Fast R-CNN é uma melhoria significativa em relação ao R-CNN, projetada para acelerar o processo de detecção de objetos (Girshick, 2015). Em vez de extrair e classificar cada região proposta separadamente, o Fast R-CNN processa a imagem completa através de uma rede convolucional, gerando um mapa de características. As regiões de interesse (ROIs), geradas pelo Selective Search ou outro método, são então projetadas nesse mapa de características. Em vez de redimensionar as regiões, como no R-CNN original, o Fast R-CNN usa uma camada chamada ROI Pooling para extrair uma representação de tamanho fixo para cada ROI. Essas representações são passadas por camadas totalmente conectadas para classificação e regressão de caixas delimitadoras, tornando o processo mais rápido e eficiente.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/fast_rcnn.png?raw=true\" width=700></center>\n",
        "\n",
        "A função de perda do Fast R-CNN também combina duas partes: uma perda de classificação e uma perda de regressão para as caixas delimitadoras, semelhante ao R-CNN. A perda de classificação usa entropia cruzada para avaliar a precisão da previsão da classe dos objetos em cada região de interesse (ROI). A perda de regressão, por sua vez, utiliza uma smooth L1 loss para refinar as coordenadas das caixas delimitadoras, tornando o treinamento mais estável ao lidar com grandes variações de valores. Essas perdas são otimizadas conjuntamente, permitindo que o Fast R-CNN seja eficiente tanto em classificar objetos quanto em prever suas localizações com precisão."
      ],
      "metadata": {
        "id": "pDa7Azfraz7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Faster R-CNN (2015)\n",
        "\n",
        "Faster R-CNN é uma evolução do Fast R-CNN, introduzida por Ren et al (2015), que elimina a dependência de métodos heurísticos como o Selective Search para gerar regiões propostas. Em vez disso, ele introduz a Region Proposal Network (RPN), uma rede neural que é treinada para gerar as regiões de interesse diretamente a partir do mapa de características da imagem. A RPN analisa janelas deslizantes sobre o mapa de características e prevê a probabilidade de cada janela conter um objeto, além de ajustar as caixas delimitadoras. Essas regiões propostas pela RPN são então refinadas e classificadas pela segunda parte do modelo, similar ao Fast R-CNN. Ao tornar a proposta de regiões uma parte treinável da rede, o Faster R-CNN é muito mais rápido e preciso, unificando a detecção de objetos em uma única arquitetura eficiente.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/faster_rcnn.png?raw=true\" width=400></center>\n",
        "\n",
        "A função de perda da Faster R-CNN combina duas partes: a perda da Region Proposal Network (RPN) e a da rede de detecção. A perda da RPN inclui uma entropia cruzada binária para classificar se as anchors contêm objetos, definido a partir da intersecção sobre a união (IoU), e uma smooth L1 loss para ajustar as caixas propostas. Na rede de detecção, utiliza-se uma entropia cruzada para classificar os objetos nas regiões de interesse (ROIs) e uma smooth L1 loss adicional para refinar as coordenadas das caixas delimitadoras. Essas perdas são combinadas para otimizar simultaneamente a proposta de regiões e a precisão na detecção de objetos."
      ],
      "metadata": {
        "id": "0xF4d_0na_f8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### YOLO (2015 - ...)\n",
        "\n",
        "YOLO (You Only Look Once) é uma abordagem inovadora para detecção de objetos que se diferencia por tratar a tarefa como um problema de regressão de ponta a ponta (Redmon et al, 2015). Em vez de gerar regiões de interesse e classificá-las separadamente, como nas arquiteturas baseadas em R-CNN, o YOLO divide a imagem em uma grade e, para cada célula, prevê diretamente as classes de objetos e as caixas delimitadoras. Isso permite que toda a imagem seja processada em uma única passagem pela rede, tornando o YOLO extremamente rápido em comparação com métodos tradicionais. Embora tenha uma troca inicial entre velocidade e precisão, versões mais recentes de YOLO têm melhorado significativamente o desempenho em ambos os aspectos, sendo amplamente utilizado em aplicações em tempo real.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/yolo.png?raw=true\" width=500></center>\n",
        "\n",
        "A função de perda do YOLO é composta por três partes principais: a perda de localização, que utiliza mean squared error (MSE) para ajustar as coordenadas das caixas delimitadoras preditas em relação às reais; a perda de confiança, que também usa MSE para avaliar a precisão na previsão da presença de objetos nas células da grade; e a perda de classificação, que aplica entropia cruzada para penalizar previsões incorretas das classes dos objetos. Essas perdas são ponderadas e combinadas para otimizar simultaneamente a localização, a presença e a classificação dos objetos detectados.\n",
        "\n",
        "#### Evoluções\n",
        "\n",
        "As evoluções do YOLO (You Only Look Once) visam aprimorar tanto a precisão quanto a eficiência da detecção de objetos. Desde o YOLO original, várias versões têm sido desenvolvidas. O YOLOv2 introduziu melhorias como a utilização de ancoras (anchor boxes) para lidar melhor com a detecção de objetos de diferentes escalas. O YOLOv3 (Redmon, 2018) adicionou uma arquitetura de rede mais profunda com detecção em múltiplas escalas, permitindo melhor precisão em objetos pequenos. O YOLOv4 otimizou ainda mais a velocidade e a precisão, incorporando avanços como convoluções otimizadas e estratégias de regularização. Mais recentemente, o YOLOv5 e YOLOv7 seguiram a tendência de otimizar o desempenho em hardware especializado, como GPUs, trazendo melhorias em velocidade e implementação prática, tornando o modelo ainda mais adequado para aplicações em tempo real e com maior capacidade de generalização."
      ],
      "metadata": {
        "id": "vux8mz_VbFz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RetinaNet (2017)\n",
        "\n",
        "RetinaNet é uma arquitetura de detecção de objetos que introduziu a Focal Loss para lidar com o problema de desbalanceamento entre classes de fundo e classes de objetos, comum em tarefas de detecção (Lin et al, 2017). Diferente das abordagens de duas etapas, como Faster R-CNN, RetinaNet adota uma estrutura de etapa única, semelhante ao YOLO, em que as previsões de classe e localizações de caixas delimitadoras são feitas diretamente a partir de mapas de características extraídos por uma rede backbone, como a ResNet. A principal inovação é a Focal Loss, que atribui menor peso às previsões de exemplos fáceis (geralmente de fundo) e foca mais nos exemplos difíceis, melhorando a detecção de objetos pequenos e difíceis de identificar, sem comprometer a eficiência. RetinaNet equilibra precisão e velocidade, tornando-se uma alternativa eficaz para detecção de objetos em larga escala.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/retinanet.png?raw=true\" width=700></center>\n",
        "\n",
        "O RetinaNet utiliza duas funções de perda principais: a Focal Loss para a tarefa de classificação, que ajusta a entropia cruzada tradicional para lidar com o desbalanceamento entre fundo e objetos, focando mais nos exemplos difíceis e ignorando os bem classificados, e a Smooth L1 Loss para a regressão das caixas delimitadoras, que penaliza os erros de maneira suave, proporcionando um ajuste mais estável das caixas. Essas duas funções são combinadas para otimizar tanto a precisão da classificação quanto a exatidão da localização dos objetos detectados.\n",
        "\n",
        "#### Feature Pyramid Network (2016)\n",
        "\n",
        "A Feature Pyramid Network (FPN) utilizada no RetinaNet é um componente chave que melhora a detecção de objetos em diferentes escalas (Lin et al, 2016). A FPN é uma arquitetura de rede projetada para construir uma representação em múltiplas resoluções da imagem de entrada, extraindo características em várias camadas, desde as mais detalhadas (para objetos pequenos) até as mais abstratas (para objetos maiores).\n",
        "\n",
        "No RetinaNet, a FPN utiliza uma abordagem \"top-down\", onde as características de camadas mais profundas e coarser (com menos resolução) são combinadas com as de camadas mais rasas e detalhadas, resultando em uma pirâmide de características que permite detectar objetos de diversos tamanhos de forma eficiente. Isso melhora significativamente a precisão em comparação com métodos que utilizam apenas a última camada da rede convolucional, tornando o RetinaNet mais eficaz para detectar objetos pequenos e em diferentes escalas."
      ],
      "metadata": {
        "id": "zcrFS4AtbO02"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DETR (2020)\n",
        "\n",
        "DETR (Detection Transformer) é uma arquitetura inovadora para detecção de objetos que combina redes neurais com o mecanismo de Transformers (Carion et al, 2020). Ao contrário das abordagens tradicionais baseadas em convoluções e region proposals, o DETR trata a detecção de objetos como um problema de correspondência entre um conjunto fixo de previsões e objetos presentes na imagem. Ele usa um backbone convolucional para extrair características da imagem, seguido por camadas de Transformers que modelam as relações globais entre as características e produzem previsões diretas para as caixas delimitadoras e classificações. DETR é único por dispensar o uso de heurísticas como Non-Maximum Suppression (NMS) e propostas de regiões, proporcionando uma abordagem simplificada e direta. Apesar de ser mais lento no treinamento inicial, o DETR demonstra forte desempenho em termos de precisão e generalização, especialmente em cenários complexos com sobreposição de objetos.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/detr_2.png?raw=true\" width=800></center>\n",
        "\n",
        "O DETR utiliza três componentes principais em sua função de perda: a Matching Loss, baseada no algoritmo de Hungarian matching, que faz a correspondência entre as previsões e os objetos reais; a perda de classificação, que usa entropia cruzada para penalizar previsões incorretas de classe; e a perda de regressão das caixas delimitadoras, que combina L1 loss para ajustar as coordenadas das caixas e generalized IoU loss para melhorar o alinhamento entre as caixas preditas e as reais. Essas perdas são otimizadas em conjunto para garantir precisão tanto na classificação quanto na localização dos objetos."
      ],
      "metadata": {
        "id": "ruIbBB82be8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Segmentação de Imagens\n",
        "\n",
        "Redes neurais para segmentação de imagens envolvem diversos componentes que trabalham em conjunto para classificar cada pixel da imagem. As camadas convolucionais são usadas para extrair características relevantes da imagem, enquanto técnicas como pooling e camadas de decodificação ajudam a capturar tanto os detalhes globais quanto os locais. Componentes como skip connections são frequentemente utilizados para preservar informações de alta resolução ao longo da rede. Além disso, as funções de perda desempenham um papel crucial ao guiar o treinamento, calculando a diferença entre a segmentação predita e a segmentação real, comumente usando métricas como cross-entropy ou IoU (Intersection over Union), ajustando os pesos da rede para melhorar a precisão. Esses componentes combinados permitem que a rede aprenda a identificar com precisão objetos dentro da imagem.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/image_segmentation.png?raw=true\" width=500></center>"
      ],
      "metadata": {
        "id": "JLV9_fVZbR5I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FCN (2015)\n",
        "\n",
        "A Fully Convolutional Network (FCN) é uma arquitetura de rede neural projetada especificamente para a segmentação de imagens. Diferente das redes tradicionais, que utilizam camadas densas no final, a FCN substitui essas camadas por camadas convolucionais, permitindo que a rede processe imagens de qualquer tamanho e preserve informações espaciais em toda a imagem. A FCN realiza segmentação pixel a pixel, transformando a última camada convolucional em um mapa de características que é posteriormente redimensionado para o tamanho original da imagem através de operações de upsampling ou deconvolution. Isso resulta em uma segmentação densa, onde cada pixel é classificado em uma determinada classe, sendo amplamente usada em aplicações que requerem uma segmentação precisa, como na área médica e na visão computacional.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/fcn.JPG?raw=true\" width=500></center>\n",
        "\n",
        "O treinamento de uma Fully Convolutional Network (FCN) segue a lógica padrão de aprendizado profundo com backpropagation, utilizando a função de perda cross-entropy pixel a pixel, que compara a predição de classe de cada pixel com o rótulo verdadeiro. A rede gera um mapa de segmentação através de camadas convolucionais, e nas camadas finais, realiza upsampling para restaurar a resolução original da imagem. Durante o treinamento, a FCN combina informações de diferentes níveis de resolução por meio de skip connections, ajustando os pesos para melhorar a precisão da segmentação. O treinamento é feito em batches de imagens, utilizando otimizadores como SGD ou Adam."
      ],
      "metadata": {
        "id": "GEvCzX7IbUmk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### U-Net (2015)\n",
        "\n",
        "A U-Net (Ronneberger et al., 2015) é uma arquitetura de rede neural convolucional projetada especificamente para segmentação de imagens, amplamente utilizada em áreas como imagem médica. Sua estrutura em forma de \"U\" é composta por um caminho de compressão (encoder) e um caminho de expansão (decoder). O encoder utiliza camadas convolucionais e operações de pooling para extrair características e reduzir a dimensionalidade da imagem, enquanto o decoder aplica camadas de upsampling para recuperar a resolução original. Um aspecto chave da U-Net são as skip connections, que conectam diretamente as camadas do encoder às camadas correspondentes do decoder, permitindo a fusão de informações detalhadas da imagem com características mais abstratas. Isso resulta em segmentações precisas, combinando contexto global e detalhes finos.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/unet.JPG?raw=true\" width=600></center>\n",
        "\n",
        "O treinamento da U-Net segue a abordagem padrão de redes neurais convolucionais, utilizando backpropagation para ajustar os pesos da rede. A U-Net é treinada com base em imagens de entrada e seus mapas de segmentação correspondentes (rótulos), e a função de perda mais comum é a cross-entropy pixel a pixel, que compara a predição de classe de cada pixel com o rótulo verdadeiro."
      ],
      "metadata": {
        "id": "ECcpT9wDb1gM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DeepLab (2015 - ...)\n",
        "\n",
        "O DeepLab (Chen et al., 2015) original foi o primeiro modelo a usar convoluções dilatadas (atrous convolutions) para capturar o contexto global da imagem sem reduzir a resolução espacial, o que foi uma inovação importante para a segmentação de imagens. Além disso, o modelo original utilizava um Conditional Random Field (CRF) como pós-processamento para refinar os resultados da segmentação, melhorando a precisão nas bordas dos objetos e suavizando os ruídos. A CRF ajuda a garantir que os contornos dos objetos segmentados se alinhem melhor com as características reais da imagem.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/deeplab.JPG?raw=true\" width=600></center>\n",
        "\n",
        "O DeepLabv2 (Chen et al., 2017) introduziu o ASPP, que aplica convoluções dilatadas com diferentes taxas em paralelo, capturando informações em múltiplas escalas. O ASPP foi um avanço crucial para lidar com objetos de tamanhos variados e melhorar a robustez da segmentação em cenários complexos.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/deeplabv2.JPG?raw=true\" width=400></center>\n",
        "\n",
        "O DeepLabv3 (Chen, 2017) manteve o uso de convoluções dilatadas e aprimorou o ASPP, tornando-o mais eficaz ao incorporar melhorias como o uso de convoluções com taxa de dilatação em todas as camadas convolucionais, em vez de apenas nas últimas. O DeepLabv3 tornou o modelo mais robusto ao variar as escalas de análise, e, nesta versão, o uso do CRF tornou-se opcional. Com as melhorias do ASPP e das convoluções dilatadas, a rede já apresentava uma segmentação precisa, reduzindo a necessidade de pós-processamento com CRF, que havia sido essencial nas primeiras versões.\n",
        "\n",
        "O treinamento do DeepLab segue a abordagem padrão de redes neurais convolucionais, utilizando backpropagation e otimização por métodos como Stochastic Gradient Descent (SGD). A rede é treinada com imagens de entrada e seus mapas de segmentação anotados, onde a função de perda, geralmente a cross-entropy pixel a pixel, calcula a diferença entre a segmentação predita e os rótulos verdadeiros."
      ],
      "metadata": {
        "id": "hP6qvI-ScAZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mask R-CNN (2017)\n",
        "\n",
        "A Mask R-CNN (He et al., 2017) é uma extensão da Faster R-CNN, projetada para realizar detecção de objetos e segmentação de instâncias simultaneamente. Além de prever as caixas delimitadoras e as classes dos objetos, como na Faster R-CNN, a Mask R-CNN adiciona uma cabeça de segmentação que gera uma máscara binária para cada objeto detectado, permitindo segmentar cada instância de forma precisa dentro de sua respectiva caixa. A arquitetura inclui uma Rede de Propostas de Região (RPN) para sugerir regiões de interesse e uma etapa de refinamento de predições usando RoIAlign, que corrige o desalinhamento causado por operações de pooling. Essa abordagem é altamente eficaz em tarefas de segmentação de instâncias e é amplamente utilizada em aplicações como visão computacional, carros autônomos e análise de imagens médicas.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/maskrcnn.JPG?raw=true\" width=600></center>\n",
        "\n",
        "O treinamento da Mask R-CNN é uma tarefa multi-objetivo que otimiza três componentes simultaneamente: detecção de objetos, regressão de caixas delimitadoras e segmentação de instâncias. A rede é treinada utilizando backpropagation com uma função de perda composta pela soma de três termos: a perda de classificação de objetos (usando cross-entropy), a perda de regressão de caixas (com smooth L1 ou L2), e a perda de segmentação de máscaras (com binary cross-entropy). O IoU (Intersection over Union) é utilizado para selecionar as propostas positivas durante o treinamento da Rede de Propostas de Região (RPN) e para definir as caixas usadas para a segmentação. Além disso, a operação RoIAlign é aplicada para garantir o correto alinhamento das regiões de interesse, permitindo que a Mask R-CNN produza predições precisas para detecção e segmentação."
      ],
      "metadata": {
        "id": "WtzGRdLNcDjK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dice Loss para Segmentação\n",
        "\n",
        "A Dice Loss foi introduzida para segmentação de imagens com o objetivo de melhorar o desempenho em cenários de grande desbalanceamento entre classes, como em imagens médicas, onde a região de interesse pode ser muito pequena em comparação ao fundo. Um dos primeiros e mais notáveis usos da Dice Loss foi no contexto de segmentação volumétrica de imagens médicas, especificamente no paper \"V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation\" (2016) de Fausto Milletari et al. A Dice Loss, derivada do coeficiente de Dice, mede a similaridade entre as predições e os rótulos verdadeiros ao focar na sobreposição entre regiões, sendo especialmente eficaz para evitar que áreas menores sejam ignoradas durante o treinamento.\n",
        "\n",
        "Ao ser utilizada em vez de, ou em conjunto com, a cross-entropy loss, a Dice Loss permitiu segmentações mais precisas em áreas pequenas e desbalanceadas, consolidando-se como uma função de perda importante para tarefas de segmentação de imagens. Sua fórmula é definida abaixo:\n",
        "\n",
        "$$\n",
        "\\text{Dice Loss} = 1 - \\frac{2 \\cdot \\sum_i^N p_i g_i}{\\sum_i^N p_i + \\sum_i^N g_i}\n",
        "$$\n",
        "\n",
        "onde $p_i$ é a previsão e $g_i$ o valor real para o pixel $i$, entre os $N$ pixels.\n",
        "\n"
      ],
      "metadata": {
        "id": "8RitbWtrXsb7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Segment Anything Model (2023)\n",
        "\n",
        "O Segment Anything Model (SAM), desenvolvido pela Meta AI (Kirillov et al., 2023), é uma rede neural projetada para realizar segmentação generalista em qualquer imagem com zero ou poucos ajustes, tornando-se uma ferramenta poderosa para uma ampla gama de aplicações. Diferente dos métodos tradicionais que exigem treinamento específico para cada tarefa ou conjunto de dados, o SAM foi treinado em um grande volume de dados com imagens e máscaras, permitindo segmentar qualquer objeto em qualquer contexto com base em prompts simples, como cliques, caixas delimitadoras ou texto. Sua arquitetura combina técnicas avançadas, como Vision Transformers (ViTs), para produzir máscaras de alta qualidade em diferentes resoluções, sem a necessidade de conhecimento específico do objeto a ser segmentado. O SAM representa um avanço na segmentação universal, democratizando o acesso a técnicas de segmentação automática para uma variedade de domínios.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/sam.JPG?raw=true\" width=700></center>"
      ],
      "metadata": {
        "id": "BMELm-PGcHLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Arquiteturas para Processamento de Linguagem Natural\n",
        "\n",
        "Temos também arquiteturas que ficaram famosas devido a seu uso em processamento de linguagem natural, como a arquitetura sequencia à sequência, e o transformer, já visto em aula anterior, e modelos recentes como BERT e GPT."
      ],
      "metadata": {
        "id": "PqC8ej35dT5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seq2Seq\n",
        "\n",
        "As arquiteturas Seq2Seq (Sequence-to-Sequence) são redes neurais projetadas para transformar uma sequência de entrada em outra sequência de saída, sendo amplamente utilizadas em tarefas como tradução automática, resumo de texto e reconhecimento de fala. A arquitetura típica envolve um encoder e um decoder, ambos normalmente compostos por redes neurais recorrentes (RNNs), como LSTMs ou GRUs, ou versões mais recentes com Transformers. O encoder processa a sequência de entrada e gera uma representação vetorial (um estado oculto), que captura as informações relevantes da entrada. O decoder, por sua vez, utiliza essa representação para gerar a sequência de saída, um elemento por vez. A transição entre o encoder e o decoder pode ser facilitada por um mecanismo de atenção, que permite que o decoder se concentre em diferentes partes da entrada enquanto gera cada elemento da saída, melhorando a precisão em tarefas complexas, como tradução.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/seq2seq.png?raw=true\" width=600></center>"
      ],
      "metadata": {
        "id": "6VQsm70EdWP6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers\n",
        "\n",
        "O Transformer é uma arquitetura baseada inteiramente em mecanismos de atenção, eliminando a necessidade de recorrência, o que permite processamento mais eficiente de sequências longas. Ele utiliza a atenção multi-head para focar em diferentes partes da entrada simultaneamente e uma estrutura de encoder-decoder para transformar sequências de entrada em saídas. Essa arquitetura revolucionou áreas como tradução automática e processamento de linguagem natural (NLP), sendo a base de modelos avançados como o BERT e GPT.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/transformer.png?raw=true\" width=400></center>"
      ],
      "metadata": {
        "id": "as4NH1pmg9np"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo de Encoder: BERT\n",
        "\n",
        "O BERT (Bidirectional Encoder Representations from Transformers) é uma aplicação avançada da arquitetura Transformer, especificamente do encoder do Transformer. Enquanto o Transformer completo possui um componente de encoder e decoder para tarefas como tradução, o BERT utiliza apenas o encoder, focando em capturar o contexto bidirecional de uma palavra em uma sequência de texto. Isso significa que BERT analisa cada palavra levando em consideração as palavras à esquerda e à direita, ao contrário de modelos unidirecionais que só olham em uma direção. O uso dessa arquitetura Transformer bidirecional permite ao BERT obter representações de texto altamente contextualizadas, tornando-o extremamente eficaz para tarefas de processamento de linguagem natural, como perguntas e respostas, reconhecimento de entidades e análise de sentimento.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/bert.jpg?raw=true\" width=700></center>"
      ],
      "metadata": {
        "id": "aVivJl1hplW2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo de Decoder: GPT\n",
        "\n",
        "O GPT-1 (Generative Pre-trained Transformer) foi o primeiro modelo da série GPT, introduzido pela OpenAI em 2018. Ele se baseia na arquitetura do decoder do Transformer, utilizando uma abordagem unidirecional para processar texto, ou seja, cada token é gerado com base apenas nos tokens anteriores da sequência. Ao contrário do BERT, que é bidirecional, o GPT-1 foca na tarefa de modelagem de linguagem em que o modelo é pré-treinado para prever a próxima palavra em uma sequência de texto. O GPT-1 foi pré-treinado em uma grande quantidade de dados não supervisionados, sendo posteriormente ajustado para tarefas específicas de NLP, como tradução e classificação, com um processo simples de fine-tuning. Sua principal inovação foi a demonstração de que o pré-treinamento de modelos de linguagem em grandes volumes de dados seguido por ajuste fino pode produzir excelentes resultados em diversas tarefas de linguagem natural.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/gpt.JPG?raw=true\" width=700></center>"
      ],
      "metadata": {
        "id": "TtmrMsgwpnM_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercícios\n",
        "\n",
        "1. Discuta sobre como diferentes componentes em arquiteturas de redes neurais avançaram a detecção de objetos e segmentação de imagens\n",
        "2. Discuta sobre como arquiteturas de transformers avançaram a área de processamento de linguagem natural.\n",
        "3. Discuta sobre como é possível utilizar algum dos conceitos desta aula para melhoria de performance preditiva a partir de modelos mais simples de classificação ou regressão."
      ],
      "metadata": {
        "id": "SGaYPySIpwUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Considerações Finais\n",
        "\n",
        "Nesta aula, vimos diversas arquiteturas especializadas para visão computacional e processamento de linguagem natural. Além disso, também discutimos sobre diferentes funções de perda e apresentamos alguns modelos com múltiplas tarefas. Isto é uma introdução para modelos mais complexos que veremos nas próximas aulas."
      ],
      "metadata": {
        "id": "-Pt1VFRGppEi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Próxima Aula\n",
        "\n",
        "Na próxima aula, veremos paradigmas avançados de aprendizagem, como:\n",
        "\n",
        "- Self-supervised Learning\n",
        "- Contrastive Learning\n",
        "- Meta-Learning"
      ],
      "metadata": {
        "id": "IMMAOMdxpr6_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referências\n",
        "\n",
        "- Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 580-587).\n",
        "- Girshick, R. (2015). Fast r-cnn. arXiv preprint arXiv:1504.08083.\n",
        "- Ren, S. et al. (2015). Faster r-cnn: Towards real-time object detection with region proposal networks. arXiv preprint arXiv:1506.01497.\n",
        "- Redmon, J., Divvala, S., Girshick, R., & Farhadi, A. (2015). You only look once: unified, real-time object detection (2015). arXiv preprint arXiv:1506.02640, 825.\n",
        "- Redmon, J. (2018). Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767.\n",
        "- Lin, T. Y., Dollár, P., Girshick, R., He, K., Hariharan, B., & Belongie, S. (2017). Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2117-2125).\n",
        "- Lin, T. (2017). Focal Loss for Dense Object Detection. arXiv preprint arXiv:1708.02002.\n",
        "- Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., & Zagoruyko, S. (2020, August). End-to-end object detection with transformers. In European conference on computer vision (pp. 213-229). Cham: Springer International Publishing.\n",
        "- Long, J., Shelhamer, E., & Darrell, T. (2015). Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 3431-3440).\n",
        "- Ronneberger, O., Fischer, P., & Brox, T. (2015). U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18 (pp. 234-241). Springer International Publishing.\n",
        "- Chen, L. C. (2014). Semantic image segmentation with deep convolutional nets and fully connected CRFs. arXiv preprint arXiv:1412.7062.\n",
        "- Chen, L. C., Papandreou, G., Kokkinos, I., Murphy, K., & Yuille, A. L. (2017). Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4), 834-848.\n",
        "- Chen, L. C. (2017). Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587.\n",
        "- He, K., Gkioxari, G., Dollár, P., & Girshick, R. (2017). Mask r-cnn. In Proceedings of the IEEE international conference on computer vision (pp. 2961-2969).\n",
        "- Milletari, F., Navab, N., & Ahmadi, S. A. (2016, October). V-net: Fully convolutional neural networks for volumetric medical image segmentation. In 2016 fourth international conference on 3D vision (3DV) (pp. 565-571). Ieee.\n",
        "- Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., ... & Girshick, R. (2023). Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 4015-4026).\n",
        "- Devlin, J. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n",
        "- Radford, A. (2018). Improving language understanding by generative pre-training."
      ],
      "metadata": {
        "id": "LUSFTFtsQ7E9"
      }
    }
  ]
}