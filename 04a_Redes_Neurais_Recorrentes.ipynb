{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNO/Q9NKOWP3loV0sYrKpCS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vhrique/anne_ptbr/blob/main/04a_Redes_Neurais_Recorrentes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Redes Neurais Recorrentes\n",
        "\n",
        "As redes neurais recorrentes (RNNs) foram desenvolvidas para lidar com dados sequenciais, como séries temporais, sinais ou texto, onde a ordem das entradas carrega significado. Diferentemente das MLPs e CNNs, que tratam as entradas de forma independente, as RNNs introduzem conexões recorrentes que permitem manter um estado interno (memória), propagando informações de passos anteriores para os seguintes. Essa característica possibilita modelar dependências temporais de curto prazo, tornando-as adequadas para tarefas como reconhecimento de fala, tradução automática e previsão de séries temporais.\n",
        "\n",
        "A estrutura básica de uma RNN consiste em uma célula recorrente que recebe a entrada atual e o estado oculto anterior, gerando uma nova representação que será usada tanto para a saída quanto para o próximo passo temporal. Essa recorrência permite compartilhar parâmetros ao longo da sequência, reduzindo a complexidade do modelo em comparação com arquiteturas totalmente conectadas aplicadas diretamente a sequências. Entretanto, RNNs tradicionais sofrem com problemas de gradientes explosivos e desvanecentes, limitando sua capacidade de capturar dependências de longo prazo. Para mitigar isso, variantes como LSTMs (Long Short-Term Memory) e GRUs (Gated Recurrent Units) foram propostas, introduzindo mecanismos de portas que regulam o fluxo de informações. Assim, as RNNs e suas variantes dominaram o processamento sequencial até a ascensão de arquiteturas baseadas em attention, como os Transformers.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/RNN.png?raw=true\" width=500></center>"
      ],
      "metadata": {
        "id": "QA3IsL6YQNUY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNNs Clássicas e Backpropagation Throught Time\n",
        "\n",
        "As RNNs clássicas processam sequências de dados com base em uma memória interna que se atualiza a cada novo passo da sequência, permitindo que a rede armazene informações sobre entradas anteriores. Isso é possível porque cada neurônio recebe, além da entrada atual, o estado oculto da etapa anterior, o que permite que as redes RNN processem uma sequência de dados etapa por etapa.\n",
        "\n",
        "O treinamento dessas redes utiliza um algoritmo conhecido como Backpropagation Through Time (BPTT), que é uma extensão do tradicional backpropagation para dados sequenciais (Werbos, 1990). No BPTT, os erros são retropropagados não apenas através das camadas da rede, mas também ao longo do tempo, de uma etapa da sequência para a anterior. Isso permite que a RNN aprenda dependências temporais, ajustando os pesos para minimizar o erro em várias etapas.\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/rnn_unfold.png?raw=true\" width=600></center>\n",
        "\n",
        "No entanto, RNNs simples podem sofrer com problemas de gradiente desvanecente, dificultando o aprendizado de dependências de longo prazo. Para lidar com isso, variantes como LSTM (Long Short-Term Memory) e GRU (Gated Recurrent Unit) foram desenvolvidas, introduzindo mecanismos que controlam o fluxo de informações e preservam a memória por períodos mais longos, tornando-as mais eficazes em capturar padrões complexos em dados sequenciais."
      ],
      "metadata": {
        "id": "PdvBrbOOQQe0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM (1997)\n",
        "\n",
        "A LSTM (Long Short-Term Memory) é uma variante avançada das RNNs, projetada para lidar com o problema do gradiente desvanecente, que afeta as redes recorrentes clássicas quando tentam aprender dependências de longo prazo. Introduzida por Hochreiter e Schmidhuber em 1997, a LSTM utiliza um sistema de portas (entrada, esquecimento e saída) que controla o fluxo de informações em cada célula da rede. A porta de esquecimento decide quais informações devem ser descartadas da memória, enquanto a porta de entrada atualiza a memória com novas informações e a porta de saída seleciona o que será passado para a próxima etapa. Esse mecanismo permite que a LSTM preserve e manipule informações ao longo de grandes intervalos de tempo, tornando-a muito eficaz para tarefas que exigem capturar dependências complexas e de longo prazo, como tradução automática, reconhecimento de fala e previsão de séries temporais. A LSTM foi um avanço crucial no campo de redes recorrentes, permitindo um aprendizado mais eficiente e robusto em sequências longas.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/lstm.png?raw=true\" width=500></center>"
      ],
      "metadata": {
        "id": "yu8g4hKfQV2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRU (2014)\n",
        "\n",
        "A GRU (Gated Recurrent Unit) é uma variante das redes LSTM, proposta por Cho et al. em 2014, que simplifica o design das LSTMs, mantendo muitos de seus benefícios. Diferente da LSTM, a GRU combina as funcionalidades das portas de entrada e de esquecimento em uma única porta de atualização, e usa uma porta de reset para controlar a quantidade de informação que flui do estado anterior para o atual. Essa simplificação torna a GRU mais eficiente em termos de computação e memória, já que ela tem menos parâmetros do que a LSTM, sem sacrificar o desempenho em muitas tarefas. A GRU é particularmente eficaz para capturar dependências temporais de curto e longo prazo, tornando-a uma escolha popular em aplicações como tradução automática, modelagem de séries temporais e reconhecimento de fala. Devido à sua simplicidade e desempenho comparável ao da LSTM, a GRU é frequentemente usada quando há necessidade de redes mais leves e rápidas.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/gru.png?raw=true\" width=400></center>"
      ],
      "metadata": {
        "id": "_LkQgCM1Qa0Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Redes Bidirecionais\n",
        "\n",
        "As BiLSTM (Bidirectional Long Short-Term Memory) e BiGRU (Bidirectional Gated Recurrent Unit) são versões bidirecionais das arquiteturas LSTM e GRU, projetadas para capturar informações tanto do passado quanto do futuro em uma sequência de dados. Em vez de processar os dados apenas em uma direção (do início ao fim), essas redes utilizam duas camadas recorrentes: uma que processa a sequência na ordem tradicional e outra que processa na ordem inversa (Schuster et al., 1997). Essa abordagem permite que as redes bidirecionais captem dependências contextuais de ambas as direções, o que é especialmente útil em tarefas como tradução automática, onde o significado de uma palavra pode depender tanto do contexto anterior quanto do posterior, ou em reconhecimento de fala, onde a compreensão de um som pode ser influenciada por fonemas adjacentes. A BiLSTM é mais flexível, devido ao seu controle mais refinado de memória com as três portas, enquanto a BiGRU, por ser mais simples e eficiente, é preferida em cenários onde o desempenho computacional é uma prioridade. Ambas são amplamente usadas em processamento de linguagem natural e tarefas sequenciais complexas.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/birnn.png?raw=true\" width=500></center>"
      ],
      "metadata": {
        "id": "ggSWJ1znjeTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seq2Seq\n",
        "\n",
        "As arquiteturas Seq2Seq (Sequence-to-Sequence) são redes neurais projetadas para transformar uma sequência de entrada em outra sequência de saída, sendo amplamente utilizadas em tarefas como tradução automática, resumo de texto e reconhecimento de fala. A arquitetura típica envolve um encoder e um decoder, ambos normalmente compostos por redes neurais recorrentes (RNNs), como LSTMs ou GRUs, ou versões mais recentes com Transformers. O encoder processa a sequência de entrada e gera uma representação vetorial (um estado oculto), que captura as informações relevantes da entrada. O decoder, por sua vez, utiliza essa representação para gerar a sequência de saída, um elemento por vez. A transição entre o encoder e o decoder pode ser facilitada por um mecanismo de atenção, que permite que o decoder se concentre em diferentes partes da entrada enquanto gera cada elemento da saída, melhorando a precisão em tarefas complexas, como tradução.\n",
        "\n",
        "<center><img src=\"https://github.com/vhrique/anne_ptbr/blob/main/figures/seq2seq.png?raw=true\" width=600></center>"
      ],
      "metadata": {
        "id": "oqmi-QKg_6eF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Desafios de Redes Neurais Recorrentes\n",
        "\n",
        "As RNNs apresentam desafios importantes, principalmente quando se trata de capturar dependências de longo prazo em dados sequenciais. Devido ao problema do gradiente desvanecente, as RNNs tendem a perder informações importantes conforme as dependências se tornam mais distantes ao longo da sequência, resultando em um aprendizado menos eficaz. Além disso, o processamento sequencial, onde cada etapa depende da anterior, torna o treinamento computacionalmente caro e difícil de paralelizar, limitando a eficiência da rede em grandes volumes de dados. Essas limitações prejudicaram o desempenho das RNNs em tarefas que requerem a análise de contextos mais longos e levaram à busca por soluções mais avançadas.\n",
        "\n"
      ],
      "metadata": {
        "id": "0NabXAVUkvvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Considerações Finais\n",
        "\n",
        "Neste capítulo, abordamos arquiteturas fundamentais de redes neurais para processamento de sequencias: RNNs, projetadas para lidar com dados sequenciais, como séries temporais e texto, capturando dependências temporais; e Transformers, que revolucionaram o processamento de linguagem natural e, mais recentemente, a visão computacional, ao usar mecanismos de atenção auto-regressiva para capturar relações globais de forma eficiente e paralelizada. Cada uma dessas arquiteturas tem aplicações específicas e vantagens únicas, sendo marcos importantes no desenvolvimento da inteligência artificial.\n",
        "\n",
        "## RNN vs Transformer\n",
        "\n",
        "As RNNs e os Transformers são ambos usados para processar dados sequenciais, mas diferem fundamentalmente em seu funcionamento. As RNNs processam a sequência de dados de forma sequencial, etapa por etapa, o que limita a eficiência e dificulta a captura de dependências de longo prazo devido ao problema do gradiente desvanecente. Já os Transformers utilizam o mecanismo de self-attention, permitindo que o modelo capture dependências em qualquer posição da sequência de forma paralelizada e mais eficiente, superando as limitações das RNNs. Como resultado, os Transformers têm se tornado o padrão em muitas tarefas de processamento de linguagem natural e visão computacional, substituindo as RNNs em diversas aplicações.\n",
        "\n",
        "## CNN vs ViT\n",
        "\n",
        "As CNNs (Redes Neurais Convolucionais) e os Vision Transformers (ViT) são ambas arquiteturas utilizadas para visão computacional, mas diferem na forma como processam imagens. As CNNs utilizam convoluções locais, aplicando filtros para detectar padrões como bordas e texturas em pequenas regiões da imagem, com forte viés indutivo sobre a estrutura espacial. Já o ViT trata a imagem como uma sequência de patches (pequenos blocos), utilizando o mecanismo de self-attention para capturar relações globais e locais de forma mais flexível. As CNNs são mais eficientes em conjuntos de dados menores, mas o ViT se destaca em grandes volumes de dados, onde pode superar as CNNs em termos de desempenho, graças à sua capacidade de modelar dependências globais de maneira eficiente.\n",
        "\n",
        "## MLP vs Transformer\n",
        "\n",
        "As MLPs (Multilayer Perceptrons) e os Transformers são ambas redes neurais, mas têm diferenças fundamentais em como processam dados. O MLP é uma arquitetura mais simples, composta por camadas totalmente conectadas que tratam cada entrada de forma independente, sem considerar a estrutura dos dados. Isso limita sua capacidade de capturar relações complexas ou estruturais entre os elementos dos dados, como sequências temporais ou imagens. Em contraste, os Transformers utilizam o mecanismo de self-attention, permitindo que o modelo capture dependências globais entre diferentes partes de uma sequência, como palavras em um texto ou patches em uma imagem, de forma mais eficiente. Enquanto os MLPs são eficientes em tarefas simples e estruturadas, os Transformers se destacam em tarefas que exigem capturar relações contextuais complexas, como processamento de linguagem natural e visão computacional.\n",
        "\n",
        "## Próxima Aula\n",
        "\n",
        "Na próxima aula, trataremos sobre paradigmas avançados de redes neurais artificiais, como aprendizagem auto-supervisionada, contrastive learning e meta-learning."
      ],
      "metadata": {
        "id": "h7m_6tpES5xC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercícios\n",
        "\n",
        "1. Verifique os exemplos da aula 4 para série temporal e processamento de linguagem natural\n",
        "2. Responda as seguintes perguntas:\n",
        "  1. Porque a LSTM foi mais eficiente que a MLP?\n",
        "  2. Qual a grande desvantagem de Transformers?\n",
        "3. Altere a LSTM por uma GRU no exemplo 4.a. Qual foi o resultado?"
      ],
      "metadata": {
        "id": "vCJSzsBOTNIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referências\n",
        "\n",
        "\n",
        "- Werbos, P. J. (1990). Backpropagation through time: what it does and how to do it. Proceedings of the IEEE, 78(10), 1550-1560.\n",
        "- Hochreiter, S. (1997). Long Short-term Memory. Neural Computation MIT-Press.\n",
        "- Cho, K. (2014). On the Properties of Neural Machine Translation: Encoder-decoder Approaches. arXiv preprint arXiv:1409.1259.\n",
        "- Schuster, M., & Paliwal, K. K. (1997). Bidirectional recurrent neural networks. IEEE transactions on Signal Processing, 45(11), 2673-2681.\n",
        "- Bahdanau, D. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.\n",
        "- Vaswani, A. (2017). Attention is all you need. Advances in Neural Information Processing Systems.\n",
        "- Ba, J. L. (2016). Layer normalization. arXiv preprint arXiv:1607.06450.\n",
        "- Devlin, J. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n",
        "- Radford, A. (2018). Improving language understanding by generative pre-training.\n",
        "- Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsupervised multitask learners. OpenAI blog, 1(8), 9.\n",
        "- Brown, T. B. (2020). Language models are few-shot learners. arXiv preprint arXiv:2005.14165.\n",
        "- Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., ... & McGrew, B. (2023). Gpt-4 technical report. arXiv preprint arXiv:2303.08774.\n",
        "- Dosovitskiy, A. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.\n",
        "- Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., & Zagoruyko, S. (2020, August). End-to-end object detection with transformers. In European conference on computer vision (pp. 213-229). Cham: Springer International Publishing.\n",
        "- Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., ... & Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 10012-10022).\n",
        "- Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., ... & Girshick, R. (2023). Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 4015-4026).\n",
        "- Wang, S., Li, B. Z., Khabsa, M., Fang, H., & Ma, H. (2020). Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768.\n",
        "- Kitaev, N., Kaiser, Ł., & Levskaya, A. (2020). Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451.\n",
        "- Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., ... & Ahmed, A. (2020). Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33, 17283-17297.\n",
        "- Dao, T., Fu, D., Ermon, S., Rudra, A., & Ré, C. (2022). Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35, 16344-16359.\n",
        "\n"
      ],
      "metadata": {
        "id": "fpwSyDT9TPKA"
      }
    }
  ]
}