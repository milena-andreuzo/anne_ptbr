{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnEcYf4Ffu3pn4CNpS5ZsA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vhrique/anne_ptbr/blob/main/08a_Novas_Fronteiras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Novas Fronteiras em Arquiteturas de Redes Neurais\n",
        "\n",
        "As novas fronteiras em arquiteturas de redes neurais refletem uma série de avanços que estão moldando o futuro da inteligência artificial. Tecnologias como o Mixture of Experts permitem que redes complexas sejam divididas em módulos especializados, otimizando a eficiência e o desempenho em tarefas diversificadas. As Deep Equilibrium Networks representam uma mudança de paradigma ao estabilizar o modelo em um ponto de equilíbrio, superando a necessidade de empilhamento de camadas profundas. Graph Neural Networks continuam a expandir suas aplicações em domínios complexos como bioinformática e redes sociais, lidando com dados estruturados e não-euclidianos. O PointNet abriu novas possibilidades no processamento de dados 3D, oferecendo uma maneira eficiente de trabalhar com nuvens de pontos, enquanto os Diffusion Models trazem avanços significativos na geração de dados realistas. NeRFs estão revolucionando a síntese de cenas 3D detalhadas a partir de imagens 2D, e as Hypernetworks introduzem uma nova flexibilidade, permitindo que uma rede neural configure os parâmetros de outra. Essas arquiteturas, juntas, representam o avanço contínuo e a exploração de novas abordagens no campo das redes neurais, trazendo soluções cada vez mais sofisticadas para desafios contemporâneos."
      ],
      "metadata": {
        "id": "7BIEtjq_uf2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combinação de Modelos com Misture of Experts\n",
        "\n",
        "O Mixture of Experts (MoE) é uma arquitetura que busca aumentar a eficiência e a capacidade de redes neurais ao dividir o aprendizado em módulos especializados. Em vez de uma única rede realizar todas as tarefas, o MoE utiliza múltiplos 'especialistas' (sub-redes), onde cada especialista é responsável por processar diferentes partes dos dados ou subtarefas. Um mecanismo de roteamento aprende a selecionar quais especialistas devem ser ativados para cada entrada, promovendo uma utilização eficiente dos recursos da rede e reduzindo o consumo computacional, especialmente em redes muito grandes. Essa abordagem permite que o modelo lide melhor com tarefas diversificadas, pois cada especialista pode focar em uma subtarefa específica, promovendo maior capacidade de generalização e melhor desempenho. O Mixture of Experts tem mostrado grande potencial em grandes modelos distribuídos, como em sistemas de processamento de linguagem natural e visão computacional, destacando-se por sua escalabilidade.\n",
        "\n",
        "O conceito de Mixture of Experts (MoE) foi introduzido por Jacobs et al. no início dos anos 90 como uma técnica de modelagem onde múltiplos especialistas, representados por sub-redes, colaboram para resolver diferentes partes de uma tarefa, com um \"gating network\" responsável por escolher qual especialista deve ser ativado para cada entrada. Originalmente, o foco estava em melhorar a performance em tarefas de aprendizado supervisionado, especialmente na classificação de dados complexos. Recentemente, o MoE ressurgiu com grande força em modelos de larga escala, especialmente em Processamento de Linguagem Natural (PLN) e Visão Computacional. Arquiteturas como a GPT-3 (Brown et al., 2020) e outras redes gigantescas têm utilizado variações do MoE para reduzir a carga computacional, mantendo um alto desempenho, ao ativar seletivamente apenas partes da rede para cada entrada, ao invés de processar todas as informações em todos os parâmetros. Isso permite a escalabilidade em modelos com trilhões de parâmetros, viabilizando a construção de redes neurais cada vez maiores e mais eficientes.\n"
      ],
      "metadata": {
        "id": "KzZF66G7ukqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Novas Redes Neurais - Deep Equilibrium Models\n",
        "\n",
        "As Deep Equilibrium Models (DEQs) representam uma abordagem inovadora em redes neurais, onde a profundidade explícita das camadas é substituída por um modelo que busca um ponto de equilíbrio. Introduzidas por Bai et al. em 2019, as DEQs eliminam a necessidade de empilhar várias camadas sequenciais, como nas arquiteturas tradicionais, e, em vez disso, iteram o mesmo conjunto de funções até que o modelo alcance um estado de equilíbrio, ou seja, até que as ativações das camadas estabilizem. Esse ponto de equilíbrio é tratado como a saída da rede. Essa característica oferece várias vantagens, como a redução do uso de memória durante o treinamento, pois a retropropagação é realizada diretamente sobre o estado de equilíbrio. Além disso, o modelo consegue atingir maior expressividade sem aumentar a complexidade de profundidade, permitindo que DEQs sejam escaláveis e eficientes, especialmente em tarefas de sequência longa, como em modelos de séries temporais e processamento de linguagem natural."
      ],
      "metadata": {
        "id": "aS_ER3n5um2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avançando Modelagem de Sequências com State-space Models\n",
        "\n",
        "Os State-Space Models (SSMs) são uma classe de modelos matemáticos que descrevem sistemas dinâmicos em termos de variáveis de estado e equações que governam sua evolução ao longo do tempo. No contexto de redes neurais, os SSMs têm sido adaptados para capturar a dinâmica temporal em dados sequenciais de forma mais eficiente, especialmente quando comparados a redes recorrentes tradicionais como LSTMs e GRUs. Esses modelos conseguem modelar a dependência de longo prazo em séries temporais ao utilizar representações em espaço de estados, onde a informação do passado é mantida em um vetor de estado que evolui de acordo com uma equação de transição. Recentemente, avanços como o modelo Mamba (Gu & Dao, 2023) têm impulsionado o uso de SSMs no aprendizado profundo, combinando essas representações com mecanismos modernos de aprendizado para otimizar a captura de padrões temporais de forma escalável e eficiente. O Mamba é um exemplo notável que integra SSMs com técnicas avançadas de processamento para melhorar a performance em tarefas complexas, como previsões de séries temporais e aprendizado de longo prazo, mostrando o potencial renovado desses modelos no campo das redes neurais."
      ],
      "metadata": {
        "id": "o2Sp65rOPUlz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trabalhando com Grafos usando Graph Neural Networks\n",
        "\n",
        "As Graph Neural Networks (GNNs) são uma poderosa classe de redes neurais projetadas para lidar com dados que têm uma estrutura de grafo, como redes sociais, moléculas químicas e sistemas de recomendação. Ao contrário das redes neurais tradicionais, que operam em dados estruturados como sequências ou grades, as GNNs são capazes de capturar relações complexas entre nós conectados por arestas, permitindo que a rede aprenda representações ricas das interações entre os elementos de um grafo. Introduzidas inicialmente por Scarselli et al. em 2009, as GNNs evoluíram rapidamente com o desenvolvimento de variações modernas como Graph Convolutional Networks (GCNs - ver Zang et al., 2019) e Graph Attention Networks (GATs - Veličković et al, 2017), que aplicam convoluções ou mecanismos de atenção sobre as conexões do grafo. Isso possibilita que as GNNs capturem dependências de longo alcance entre nós, tornando-as extremamente eficazes para tarefas como classificação de nós, predição de links e geração de grafos. Hoje, as GNNs são amplamente utilizadas em diversas áreas, como biologia computacional, química quântica, e redes de conhecimento, demonstrando seu valor em modelar dados estruturados complexos."
      ],
      "metadata": {
        "id": "kv6XHiAwuqJp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trabalhando com Nuvem de Pontos utilizando PointNet\n",
        "\n",
        "O PointNet é uma arquitetura inovadora projetada para processar diretamente dados 3D em forma de nuvem de pontos, rompendo com a abordagem tradicional de transformar esses dados em grades volumétricas ou projeções em imagens. Introduzido por Qi et al. em 2017, o PointNet utiliza uma rede neural que processa individualmente cada ponto da nuvem, seguido de uma operação de agregação simétrica, como max pooling, que captura as características globais da nuvem. Essa estratégia garante a invariância à permutação dos pontos, uma característica essencial para lidar com dados 3D, onde a ordem dos pontos não deve afetar a representação final. Além disso, ao focar diretamente nos pontos em si, o PointNet consegue ser eficiente e escalável, permitindo sua aplicação em tarefas como classificação de objetos 3D, segmentação de cenas e detecção de partes. A simplicidade e eficácia dessa arquitetura impulsionaram novos avanços na área de visão computacional 3D, abrindo caminho para o desenvolvimento de modelos ainda mais complexos e robustos para lidar com ambientes tridimensionais."
      ],
      "metadata": {
        "id": "uK4OiQNkuvwc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gerando Imagens com Diffusion\n",
        "\n",
        "Os Diffusion Models (Ho et al., 2020) são uma classe emergente de modelos generativos que têm ganhado destaque recentemente por sua capacidade de gerar dados de alta qualidade, como imagens, sons e outros tipos de informações complexas. Esses modelos operam ao aprender como reverter um processo de difusão gradual, onde os dados são corrompidos com ruído de forma incremental até se tornarem puro ruído. Durante o treinamento, o modelo aprende a restaurar os dados originais a partir de diferentes níveis de ruído, permitindo que ele gere amostras realistas ao reverter esse processo passo a passo. Introduzidos inicialmente no contexto de aprendizado não supervisionado, os Diffusion Models têm demonstrado uma performance impressionante em comparação com outras abordagens generativas, como GANs e VAEs, especialmente em tarefas que exigem alta fidelidade e detalhamento, como síntese de imagens. Recentemente, variações desses modelos têm sido exploradas em diversas aplicações, mostrando grande potencial em áreas como geração de conteúdo visual e simulações físicas."
      ],
      "metadata": {
        "id": "PI70Xfsauy1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gerando Ambientes 3D com Neural Radiance Fields e 3D Gaussian Splatting\n",
        "\n",
        "Neural Radiance Fields (NeRF) é uma técnica revolucionária de síntese de cenas 3D que gera imagens foto-realistas a partir de um conjunto de imagens 2D capturadas de diferentes ângulos. Introduzido por Mildenhall et al. em 2020, o NeRF utiliza uma rede neural para modelar a densidade volumétrica e a emissão de luz de cada ponto em um espaço tridimensional contínuo. Ao treinar a rede com imagens e suas respectivas posições de câmera, o NeRF aprende a reconstruir o campo de radiação da cena, permitindo renderizações precisas de novas perspectivas que não estavam presentes no conjunto de treinamento. Essa abordagem é notável por sua eficiência e pela capacidade de gerar detalhes finos, como texturas e variações de iluminação, sem a necessidade de um grande número de parâmetros ou transformações complexas de dados 3D. O NeRF tem sido amplamente aplicado em áreas como realidade virtual, captura de ambientes reais e geração de ativos para jogos, destacando-se como uma das principais inovações em reconstrução de cenas tridimensionais.\n",
        "\n",
        "O 3D Gaussian Splatting (Kerbl et al., 2023) é uma técnica inovadora para reconstrução e renderização de cenas tridimensionais que modela a geometria e a aparência de uma cena utilizando uma distribuição de Gaussians no espaço 3D. Ao contrário de métodos tradicionais, como o NeRF, que usa redes neurais para mapear a emissão de luz e densidade volumétrica, o 3D Gaussian Splatting representa a cena como uma nuvem de pontos, onde cada ponto tem uma distribuição Gaussiana associada que define sua posição, forma, e propriedades de cor e transparência. Essas Gaussians podem ser projetadas diretamente nas imagens durante o processo de renderização, criando uma representação contínua e eficiente da cena. Essa abordagem permite renderizações rápidas e de alta qualidade, com menos custos computacionais, especialmente em comparação com métodos baseados em redes profundas. Além disso, o 3D Gaussian Splatting se destaca pela flexibilidade, sendo capaz de capturar detalhes finos da cena com alta precisão, o que o torna adequado para aplicações em gráficos 3D, realidade aumentada, e efeitos visuais em tempo real."
      ],
      "metadata": {
        "id": "a5aWcT77vFjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aprendendo Pesos de Redes Neurais com HyperNetworks\n",
        "\n",
        "As Hypernetworks são uma abordagem inovadora em redes neurais, onde uma rede neural gera os parâmetros de outra rede, em vez de usar pesos fixos aprendidos diretamente a partir dos dados. Popularizada por Ha et al. em 2016, e com idéias similares apresentadas por Schmidhuber em 1992, essa arquitetura oferece maior flexibilidade e capacidade de adaptação, permitindo que a rede principal (chamada de target network) seja parametrizada por uma rede externa (a hypernetwork). Esse processo facilita a criação de modelos mais eficientes, especialmente em cenários com múltiplas tarefas ou dados com diferentes características, pois a hypernetwork pode ajustar dinamicamente os parâmetros da target network conforme necessário. As Hypernetworks têm sido exploradas em várias áreas, como aprendizado de poucas amostras, adaptação rápida a novos domínios e otimização de redes com grande número de parâmetros, oferecendo um novo nível de personalização e generalização em redes neurais. Esse paradigma tem mostrado grande potencial em arquiteturas modernas e na pesquisa sobre como otimizar redes para diferentes contextos sem a necessidade de treinar múltiplos modelos separados."
      ],
      "metadata": {
        "id": "nYJXDRLEuibw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Considerações Finais\n",
        "\n",
        "Como vimos, as arquiteturas emergentes em redes neurais estão expandindo as fronteiras do que é possível no campo da inteligência artificial. Modelos como Mixture of Experts, Deep Equilibrium Networks, Graph Neural Networks, PointNet, Diffusion Models, NeRFs e Hypernetworks não apenas representam avanços em eficiência computacional e escalabilidade, mas também trazem soluções inovadoras para problemas complexos em diferentes domínios, como processamento de linguagem, visão computacional e modelagem tridimensional. Essas arquiteturas são o reflexo de um movimento contínuo para superar as limitações das redes neurais tradicionais, abrindo caminho para aplicações cada vez mais sofisticadas e exigentes. Ao explorar essas novas fronteiras, pesquisadores e profissionais têm a oportunidade de não só melhorar o estado da arte em IA, mas também transformar a maneira como a tecnologia é aplicada no mundo real."
      ],
      "metadata": {
        "id": "JH6FxdY7vRiv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referências\n",
        "\n",
        "- Jacobs, R. A., Jordan, M. I., Nowlan, S. J., & Hinton, G. E. (1991). Adaptive mixtures of local experts. Neural computation, 3(1), 79-87.\n",
        "- Brown, T. B. (2020). Language models are few-shot learners. arXiv preprint arXiv:2005.14165.\n",
        "- Bai, S., Kolter, J. Z., & Koltun, V. (2019). Deep equilibrium models. Advances in neural information processing systems, 32.\n",
        "- Gu, A., & Dao, T. (2023). Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752.\n",
        "- Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., & Monfardini, G. (2008). The graph neural network model. IEEE transactions on neural networks, 20(1), 61-80.\n",
        "- Zhang, S., Tong, H., Xu, J., & Maciejewski, R. (2019). Graph convolutional networks: a comprehensive review. Computational Social Networks, 6(1), 1-23.\n",
        "- Veličković, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., & Bengio, Y. (2017). Graph attention networks. arXiv preprint arXiv:1710.10903.\n",
        "- Qi, C. R., Su, H., Mo, K., & Guibas, L. J. (2017). Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 652-660).\n",
        "- Qi, C. R., Yi, L., Su, H., & Guibas, L. J. (2017). Pointnet++: Deep hierarchical feature learning on point sets in a metric space. Advances in neural information processing systems, 30.\n",
        "- Ho, J., Jain, A., & Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in neural information processing systems, 33, 6840-6851.\n",
        "- Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., & Ng, R. (2020). Nerf: representing scenes as neural radiance fields for view synthesis (2020). arXiv preprint arXiv:2003.08934.\n",
        "- Kerbl, B., Kopanas, G., Leimkühler, T., & Drettakis, G. (2023). 3D Gaussian Splatting for Real-Time Radiance Field Rendering. ACM Trans. Graph., 42(4), 139-1.\n",
        "- Ha, D., Dai, A., & Le, Q. V. (2016). Hypernetworks. arXiv preprint arXiv:1609.09106.\n",
        "- Schmidhuber, J. (1992). Learning to control fast-weight memories: An alternative to dynamic recurrent networks. Neural Computation, 4(1), 131-139."
      ],
      "metadata": {
        "id": "n4ZBPPy4vTFF"
      }
    }
  ]
}