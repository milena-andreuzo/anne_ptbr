{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORYs8FXi8P3ro9UN+grl/J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vhrique/anne_ptbr/blob/main/08a_Novas_Fronteiras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Novas Fronteiras em Arquiteturas de Redes Neurais\n",
        "\n",
        "As novas fronteiras em arquiteturas de redes neurais refletem uma série de avanços que estão moldando o futuro da inteligência artificial. Tecnologias como o Mixture of Experts permitem que redes complexas sejam divididas em módulos especializados, otimizando a eficiência e o desempenho em tarefas diversificadas. As Deep Equilibrium Networks representam uma mudança de paradigma ao estabilizar o modelo em um ponto de equilíbrio, superando a necessidade de empilhamento de camadas profundas. Graph Neural Networks continuam a expandir suas aplicações em domínios complexos como bioinformática e redes sociais, lidando com dados estruturados e não-euclidianos. O PointNet abriu novas possibilidades no processamento de dados 3D, oferecendo uma maneira eficiente de trabalhar com nuvens de pontos, enquanto os Diffusion Models trazem avanços significativos na geração de dados realistas. NeRFs estão revolucionando a síntese de cenas 3D detalhadas a partir de imagens 2D, e as Hypernetworks introduzem uma nova flexibilidade, permitindo que uma rede neural configure os parâmetros de outra. Essas arquiteturas, juntas, representam o avanço contínuo e a exploração de novas abordagens no campo das redes neurais, trazendo soluções cada vez mais sofisticadas para desafios contemporâneos."
      ],
      "metadata": {
        "id": "7BIEtjq_uf2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combinação de Modelos com Misture of Experts\n",
        "\n",
        "O Mixture of Experts (MoE) é uma arquitetura que busca aumentar a eficiência e a capacidade de redes neurais ao dividir o aprendizado em módulos especializados. Em vez de uma única rede realizar todas as tarefas, o MoE utiliza múltiplos 'especialistas' (sub-redes), onde cada especialista é responsável por processar diferentes partes dos dados ou subtarefas. Um mecanismo de roteamento aprende a selecionar quais especialistas devem ser ativados para cada entrada, promovendo uma utilização eficiente dos recursos da rede e reduzindo o consumo computacional, especialmente em redes muito grandes. Essa abordagem permite que o modelo lide melhor com tarefas diversificadas, pois cada especialista pode focar em uma subtarefa específica, promovendo maior capacidade de generalização e melhor desempenho. O Mixture of Experts tem mostrado grande potencial em grandes modelos distribuídos, como em sistemas de processamento de linguagem natural e visão computacional, destacando-se por sua escalabilidade.\n",
        "\n",
        "O conceito de Mixture of Experts (MoE) foi introduzido por Jacobs et al. no início dos anos 90 como uma técnica de modelagem onde múltiplos especialistas, representados por sub-redes, colaboram para resolver diferentes partes de uma tarefa, com um \"gating network\" responsável por escolher qual especialista deve ser ativado para cada entrada. Originalmente, o foco estava em melhorar a performance em tarefas de aprendizado supervisionado, especialmente na classificação de dados complexos. Recentemente, o MoE ressurgiu com grande força em modelos de larga escala, especialmente em Processamento de Linguagem Natural (PLN) e Visão Computacional. Arquiteturas como a GPT-3 (Brown et al., 2020) e outras redes gigantescas têm utilizado variações do MoE para reduzir a carga computacional, mantendo um alto desempenho, ao ativar seletivamente apenas partes da rede para cada entrada, ao invés de processar todas as informações em todos os parâmetros. Isso permite a escalabilidade em modelos com trilhões de parâmetros, viabilizando a construção de redes neurais cada vez maiores e mais eficientes.\n"
      ],
      "metadata": {
        "id": "KzZF66G7ukqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Novas Redes Neurais - Deep Equilibrium Models\n",
        "\n",
        "As Deep Equilibrium Models (DEQs) representam uma abordagem inovadora em redes neurais, onde a profundidade explícita das camadas é substituída por um modelo que busca um ponto de equilíbrio. Introduzidas por Bai et al. em 2019, as DEQs eliminam a necessidade de empilhar várias camadas sequenciais, como nas arquiteturas tradicionais, e, em vez disso, iteram o mesmo conjunto de funções até que o modelo alcance um estado de equilíbrio, ou seja, até que as ativações das camadas estabilizem. Esse ponto de equilíbrio é tratado como a saída da rede. Essa característica oferece várias vantagens, como a redução do uso de memória durante o treinamento, pois a retropropagação é realizada diretamente sobre o estado de equilíbrio. Além disso, o modelo consegue atingir maior expressividade sem aumentar a complexidade de profundidade, permitindo que DEQs sejam escaláveis e eficientes, especialmente em tarefas de sequência longa, como em modelos de séries temporais e processamento de linguagem natural."
      ],
      "metadata": {
        "id": "aS_ER3n5um2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avançando Modelagem de Sequências com State-space Models\n",
        "\n",
        "Os State-Space Models (SSMs) são uma classe de modelos matemáticos que descrevem sistemas dinâmicos em termos de variáveis de estado e equações que governam sua evolução ao longo do tempo. No contexto de redes neurais, os SSMs têm sido adaptados para capturar a dinâmica temporal em dados sequenciais de forma mais eficiente, especialmente quando comparados a redes recorrentes tradicionais como LSTMs e GRUs. Esses modelos conseguem modelar a dependência de longo prazo em séries temporais ao utilizar representações em espaço de estados, onde a informação do passado é mantida em um vetor de estado que evolui de acordo com uma equação de transição. Recentemente, avanços como o modelo Mamba (Gu & Dao, 2023) têm impulsionado o uso de SSMs no aprendizado profundo, combinando essas representações com mecanismos modernos de aprendizado para otimizar a captura de padrões temporais de forma escalável e eficiente. O Mamba é um exemplo notável que integra SSMs com técnicas avançadas de processamento para melhorar a performance em tarefas complexas, como previsões de séries temporais e aprendizado de longo prazo, mostrando o potencial renovado desses modelos no campo das redes neurais."
      ],
      "metadata": {
        "id": "o2Sp65rOPUlz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trabalhando com Grafos usando Graph Neural Networks\n",
        "\n",
        "As Graph Neural Networks (GNNs) são uma poderosa classe de redes neurais projetadas para lidar com dados que têm uma estrutura de grafo, como redes sociais, moléculas químicas e sistemas de recomendação. Ao contrário das redes neurais tradicionais, que operam em dados estruturados como sequências ou grades, as GNNs são capazes de capturar relações complexas entre nós conectados por arestas, permitindo que a rede aprenda representações ricas das interações entre os elementos de um grafo. Introduzidas inicialmente por Scarselli et al. em 2009, as GNNs evoluíram rapidamente com o desenvolvimento de variações modernas como Graph Convolutional Networks (GCNs) e Graph Attention Networks (GATs), que aplicam convoluções ou mecanismos de atenção sobre as conexões do grafo. Isso possibilita que as GNNs capturem dependências de longo alcance entre nós, tornando-as extremamente eficazes para tarefas como classificação de nós, predição de links e geração de grafos. Hoje, as GNNs são amplamente utilizadas em diversas áreas, como biologia computacional, química quântica, e redes de conhecimento, demonstrando seu valor em modelar dados estruturados complexos."
      ],
      "metadata": {
        "id": "kv6XHiAwuqJp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trabalhando com Nuvem de Pontos utilizando PointNet"
      ],
      "metadata": {
        "id": "uK4OiQNkuvwc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gerando Imagens com Diffusion"
      ],
      "metadata": {
        "id": "PI70Xfsauy1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gerando Ambientes 3D com Neural Radiance Fields e 3D Gaussian Splatting"
      ],
      "metadata": {
        "id": "a5aWcT77vFjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aprendendo Pesos de Redes Neurais com HyperNetworks"
      ],
      "metadata": {
        "id": "nYJXDRLEuibw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Considerações Finais"
      ],
      "metadata": {
        "id": "JH6FxdY7vRiv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referências\n",
        "\n",
        "- Jacobs, R. A., Jordan, M. I., Nowlan, S. J., & Hinton, G. E. (1991). Adaptive mixtures of local experts. Neural computation, 3(1), 79-87.\n",
        "- Brown, T. B. (2020). Language models are few-shot learners. arXiv preprint arXiv:2005.14165.\n",
        "- Bai, S., Kolter, J. Z., & Koltun, V. (2019). Deep equilibrium models. Advances in neural information processing systems, 32.\n",
        "- Gu, A., & Dao, T. (2023). Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752.\n",
        "- Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., & Monfardini, G. (2008). The graph neural network model. IEEE transactions on neural networks, 20(1), 61-80."
      ],
      "metadata": {
        "id": "n4ZBPPy4vTFF"
      }
    }
  ]
}